{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to VC3 Documentation\n\n\nFor portal access visit \nhttps://www.virtualclusters.org\n.\n\n\n\n\nMay 23, 2018. The \nVC3 project\n is searching for users and HPC resource providers to evaluate its closed-beta (or \"limited\") release.  During the limited beta release there might be system bugs, instability and lack of availability. \n\n\n\n\n\n\nThe limited beta user invitation signup is \nhere\n. \n\n\nThe limited beta HPC resource invitation is \nhere\n.  Note you do not necessarily need to have an institutional affiliation with the HPC resource organization, but you should be authorized to run jobs there.  \n\n\n\n\nGetting Started With VC3\n\n\nVirtual Clusters for Community Computation, or VC3, is a platform for connecting\nclusters, grids, and clouds. VC3 can run overlay systems for a variety of\ncluster frameworks to make disparate resources appear as a single \u201cvirtual\u201d\nresource for collaborative science.\n\n\nVC3 User Guide\n\n\nAll users of VC3 should read this portion of the documentation. This provides examples and documentation around VC3\u2019s assortment of options and how to specify them on the portal.\n\n\nGlossary\n\n\nThis section addresses a variety of specific terms used throughout this project\nand documentation.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-vc3-documentation", 
            "text": "For portal access visit  https://www.virtualclusters.org .   May 23, 2018. The  VC3 project  is searching for users and HPC resource providers to evaluate its closed-beta (or \"limited\") release.  During the limited beta release there might be system bugs, instability and lack of availability.     The limited beta user invitation signup is  here .   The limited beta HPC resource invitation is  here .  Note you do not necessarily need to have an institutional affiliation with the HPC resource organization, but you should be authorized to run jobs there.", 
            "title": "Welcome to VC3 Documentation"
        }, 
        {
            "location": "/#getting-started-with-vc3", 
            "text": "Virtual Clusters for Community Computation, or VC3, is a platform for connecting\nclusters, grids, and clouds. VC3 can run overlay systems for a variety of\ncluster frameworks to make disparate resources appear as a single \u201cvirtual\u201d\nresource for collaborative science.", 
            "title": "Getting Started With VC3"
        }, 
        {
            "location": "/#vc3-user-guide", 
            "text": "All users of VC3 should read this portion of the documentation. This provides examples and documentation around VC3\u2019s assortment of options and how to specify them on the portal.", 
            "title": "VC3 User Guide"
        }, 
        {
            "location": "/#glossary", 
            "text": "This section addresses a variety of specific terms used throughout this project\nand documentation.", 
            "title": "Glossary"
        }, 
        {
            "location": "/userguide/gettingstarted/", 
            "text": "VC3 User Guide\n\n\nBuilding Your First Virtual Cluster\n\n\nPrerequisites\n\n\nIn order to use VC3, you\u2019ll need an allocation or account in a target resource that is supported by VC3. These include, but are not limited to:\n\n\n\n\nUniversity of Chicago - Research Computing Center\n\n\nUniversity of Notre Dame - Center for Research Computing\n\n\nNational Energy Research Scientific Computing Center (NERSC)\n\n\nPittsburgh Supercomputing Center (Bridges)\n\n\nOpen Science Grid\n\n\nand more!\n\n\n\n\nInstitutions and resources are added frequently. Check the \nResources\n page for the full list of connected resources in the service.\n\n\n1. Login or Create Account\n\n\nWhen you first visit \nVirtual Clusters\n, you\u2019ll\nbe presented with a Login link in the top right of the screen. Click \u201cLogin\u201d -\nthis will take you to a \nGlobus\n sign-in site.\n\n\n\n\n2. Sign in to Globus\n\n\nYou will then be asked to sign in with your institutional identity, or your\nGlobus ID. If you are using the former, simply type in the name of your\ninstitution and click \"Continue\". Proceed to \nStep 3a\n.\n\n\nOtherwise, click \"Globus ID to sign in\" and proceed to the alternate \nStep 3b\n.\n\n\n\n\n3a. Login with your institutional ID\n\n\nYou should be presented with a login page for your institutional ID, with your\ninstitution\u2019s branding. Go ahead and sign-in now. Note that your password is not\nsent to the VC3 or Globus web portals. Continue to step 4.\n\n\n\n\n3b. Login with your Globus ID\n\n\nOnce you have clicked on the \"Globus ID to sign in\" link, you will be presented with the page below. Please, proceed to log in if you already have a globus account, or click on the upper right link to create a new globus account.\n\n\n\n\nIf you are creating a new account, a short form to fill up will be shown next (see below).\n\n\n\n\nAfter you have completed this form, a confirmation e-mail with a verification code will be sent. Copy and paste such code and click on the \"verify\" button.\n\n\n\n\nAfter your account is verified, you will be presented with the page below. Click on \"Continue\" to finalize the sign up process.\n\n\n\n\n4. Complete or update your VC3 profile\n\n\nOnce you have signed in, you\u2019ll be asked to update or complete your VC3 profile\nwith information such as your Institution and any other information we cannot\ndirectly extract from your Globus account. Click \u201cUpdate Profile\u201d once done.\n\n\nNote that other actions are not allowed until the profile is complete. \n\n\nThe \"SSH KEY\" box can be used to upload your public ssh key. \nThis will allow you to access your future clusters in the VC3 infrastructure.\nRemeber: it is the \npublic\n key what needs to be uploaded. \n\n\nIf you do not have a public ssh key, \ninstructions to create it can be found in the link at the top-right corner. \n\n\n\n\n5. Resources\n\n\nThe VC3 team curates an ever-expanding list of resources for end-users, with a\nfocus on university campus clusters, HPC centers, and cloud resources. You can find these\nresources by clicking the \u201cResources\u201d link on the left panel.\n\n\nYou can also click an individual resource and see expanded information, such as\nbatch system type, links to documentation, etc.\n\n\n\n\n6. Connect an Allocation\n\n\nAfter updating your profile, you can connect an allocation to the VC3 service.\nAn allocation, in VC3, is defined as combination of a username and resource\ntarget that consumes some type of compute unit - regardless of whether it is\nbilled as Service Units (many HPC centers), dollars (AWS, GCE), or priority\n(HTCondor and other opportunistic systems).\n\n\nClicking My Allocations on the left shows all allocations currently associated\nwith your account. You may select a new one by clicking + New Allocation.\n\n\n\n\nYou will be able to select a resource target from the drop down menu, and enter\nan account name for the resource. This is the same account name that you use to\nSSH to the remote system.\n\n\n\n\nOnce you\u2019ve connected your allocation, the system will provide a set of instructions you must follow in order to validate it.\n\n\n\n\nIn order to create a virtual cluster, the VC3 software needs to be able to SSH\nto the remote resource. If you click your allocation, you should see a section\ntitled Public Token.\n\n\n\n\n\nYou will need to add this token to your Unix account, in the file\n~/.ssh/authorized_keys. You can either edit this file with your favorite editor\n(such as nano, vim, or emacs), or use the echo command to append it to the\nauthorized keys file.\n\n\n\n\nNote some resources might have special instructions \nin order to add public tokens to your account, in which case a link to \nthe resource instructions to follow will be displayed in this section.\n\n\nThis token allows the VC3 system to SSH into a cluster as yourself and submit\njobs on your, or your project\u2019s, behalf.\n\n\nOnce the ssh key has been placed in the resource, click on the \"validate\" button (step 4).\nYou will notice the progress bar at the top changes color to green with a message \"Ready\"\nwhen VC3 has complete the validation. Now the allocation is ready to be used. \n\n\n7. Defining a Project\n\n\nVC3, as a platform for cooperative scientific computing, allows you create\nprojects to share your allocations and virtual clusters with trusted members of\nyour group, laboratory, or collaboration. To start a new project, click\n\u201cProjects\u201d on the sidebar, then click \u201c+ New Project\u201d.\n\n\n\n\nYou may give your project an aribtrary name and choose initial project members and/or allocations.\nYou are, by default, member of all your projects.\nOnce finished, click \u201cCreate Project\u201d.\n\n\n\n\nYou should be returned to the Projects page, where you will be able to see all\nof your projects and memberships.\nYou can access the configuration of each project in the table by clicking on its name.\nThis way you can add or remove members and/or allocations to a given project.\n\n\n\n\n8. Creating a Cluster Template\n\n\nVC3 allows users to create \u201cCluster Templates\u201d that describe the components of\ntheir virtual cluster, including number of head nodes, worker nodes, etc. We\ncurrently support HTCondor and WorkQueue clusters with dynamic worker nodes,\nand fixed head nodes.\n\n\nTo define a new template, click the \u201cCluster Templates\u201d link on the left panel.\nYou\u2019ll be able to give your cluster a name, select framework, and number of\nworkers. Click \u201cDefine Cluster\u201d to finish creating the template.\n\n\nYou should be returned to the Cluster Templates page, where you will be able to see all of your templates.\nYou can access the configuration of each cluster template in the table by clicking on its name.\n\n\n\n\n9. Environments\n\n\nAn Enviroment is a collection of software packages to be included in a virtual cluster. \nClick on \"+New Environment\" to create new ones. \nFrom the \"PACKAGE LIST\" menu you can select one or more packages to be included in your new Enviroment.\n\n\nFrom the second menu you can choose, if needed, the Operative System. If the chosen Operative System\nis not available natively, the system will attempt to provide it via containers.\nPlease, look into the \nresources page\n to check if the\nresource you are planning to use with this environment has container options available in the \n\"features\" section (E.g: singularity).\n\n\n\n\nOnce done, click on the \"Create New Enviroment\" button.\nYou should be returned to the Enviroments page, where you will be able to see all of your environments.\nYou can access the configuration of each environment in the table by clicking on its name.\n\n\n10. Launching a Virtual Cluster\n\n\nOnce you are part of a project, which has associated allocations,\nyou may launch a Virtual Cluster. In order to do so, click on\n\"New Virtual Cluster\", found in the Virtual Clusters tab from the left nav-menu.\n\n\n\n\nFrom here, you are prompted to select the project that you would like to use in\norder to launch a Virtual Cluster. Once selected, press 'Next'.\n\n\n\n\nGive your Virtual Cluster a name, select the cluster template, environment (optional) and allocation.\nYou may specify how long you would like your Virtual Cluster to run for. If not\nspecified, the expiration defaults to 72 hours. Which means that after 72 hours,\nyour Virtual Cluster will automatically begin to terminate itself.\n\n\n\n\nOnce launched, you will be redirected to your Virtual Cluster's detailed page,\nwhere you may track the status if your Virtual Cluster. The head node should take\na few moments to configure. Once complete, you will be prompted with instructions\non how to access the head node.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/userguide/gettingstarted/#vc3-user-guide", 
            "text": "", 
            "title": "VC3 User Guide"
        }, 
        {
            "location": "/userguide/gettingstarted/#building-your-first-virtual-cluster", 
            "text": "Prerequisites  In order to use VC3, you\u2019ll need an allocation or account in a target resource that is supported by VC3. These include, but are not limited to:   University of Chicago - Research Computing Center  University of Notre Dame - Center for Research Computing  National Energy Research Scientific Computing Center (NERSC)  Pittsburgh Supercomputing Center (Bridges)  Open Science Grid  and more!   Institutions and resources are added frequently. Check the  Resources  page for the full list of connected resources in the service.", 
            "title": "Building Your First Virtual Cluster"
        }, 
        {
            "location": "/userguide/gettingstarted/#1-login-or-create-account", 
            "text": "When you first visit  Virtual Clusters , you\u2019ll\nbe presented with a Login link in the top right of the screen. Click \u201cLogin\u201d -\nthis will take you to a  Globus  sign-in site.", 
            "title": "1. Login or Create Account"
        }, 
        {
            "location": "/userguide/gettingstarted/#2-sign-in-to-globus", 
            "text": "You will then be asked to sign in with your institutional identity, or your\nGlobus ID. If you are using the former, simply type in the name of your\ninstitution and click \"Continue\". Proceed to  Step 3a .  Otherwise, click \"Globus ID to sign in\" and proceed to the alternate  Step 3b .", 
            "title": "2. Sign in to Globus"
        }, 
        {
            "location": "/userguide/gettingstarted/#3a-login-with-your-institutional-id", 
            "text": "You should be presented with a login page for your institutional ID, with your\ninstitution\u2019s branding. Go ahead and sign-in now. Note that your password is not\nsent to the VC3 or Globus web portals. Continue to step 4.", 
            "title": "3a. Login with your institutional ID"
        }, 
        {
            "location": "/userguide/gettingstarted/#3b-login-with-your-globus-id", 
            "text": "Once you have clicked on the \"Globus ID to sign in\" link, you will be presented with the page below. Please, proceed to log in if you already have a globus account, or click on the upper right link to create a new globus account.   If you are creating a new account, a short form to fill up will be shown next (see below).   After you have completed this form, a confirmation e-mail with a verification code will be sent. Copy and paste such code and click on the \"verify\" button.   After your account is verified, you will be presented with the page below. Click on \"Continue\" to finalize the sign up process.", 
            "title": "3b. Login with your Globus ID"
        }, 
        {
            "location": "/userguide/gettingstarted/#4-complete-or-update-your-vc3-profile", 
            "text": "Once you have signed in, you\u2019ll be asked to update or complete your VC3 profile\nwith information such as your Institution and any other information we cannot\ndirectly extract from your Globus account. Click \u201cUpdate Profile\u201d once done.  Note that other actions are not allowed until the profile is complete.   The \"SSH KEY\" box can be used to upload your public ssh key. \nThis will allow you to access your future clusters in the VC3 infrastructure.\nRemeber: it is the  public  key what needs to be uploaded.   If you do not have a public ssh key, \ninstructions to create it can be found in the link at the top-right corner.", 
            "title": "4. Complete or update your VC3 profile"
        }, 
        {
            "location": "/userguide/gettingstarted/#5-resources", 
            "text": "The VC3 team curates an ever-expanding list of resources for end-users, with a\nfocus on university campus clusters, HPC centers, and cloud resources. You can find these\nresources by clicking the \u201cResources\u201d link on the left panel.  You can also click an individual resource and see expanded information, such as\nbatch system type, links to documentation, etc.", 
            "title": "5. Resources"
        }, 
        {
            "location": "/userguide/gettingstarted/#6-connect-an-allocation", 
            "text": "After updating your profile, you can connect an allocation to the VC3 service.\nAn allocation, in VC3, is defined as combination of a username and resource\ntarget that consumes some type of compute unit - regardless of whether it is\nbilled as Service Units (many HPC centers), dollars (AWS, GCE), or priority\n(HTCondor and other opportunistic systems).  Clicking My Allocations on the left shows all allocations currently associated\nwith your account. You may select a new one by clicking + New Allocation.   You will be able to select a resource target from the drop down menu, and enter\nan account name for the resource. This is the same account name that you use to\nSSH to the remote system.   Once you\u2019ve connected your allocation, the system will provide a set of instructions you must follow in order to validate it.   In order to create a virtual cluster, the VC3 software needs to be able to SSH\nto the remote resource. If you click your allocation, you should see a section\ntitled Public Token.   You will need to add this token to your Unix account, in the file\n~/.ssh/authorized_keys. You can either edit this file with your favorite editor\n(such as nano, vim, or emacs), or use the echo command to append it to the\nauthorized keys file.   Note some resources might have special instructions \nin order to add public tokens to your account, in which case a link to \nthe resource instructions to follow will be displayed in this section.  This token allows the VC3 system to SSH into a cluster as yourself and submit\njobs on your, or your project\u2019s, behalf.  Once the ssh key has been placed in the resource, click on the \"validate\" button (step 4).\nYou will notice the progress bar at the top changes color to green with a message \"Ready\"\nwhen VC3 has complete the validation. Now the allocation is ready to be used.", 
            "title": "6. Connect an Allocation"
        }, 
        {
            "location": "/userguide/gettingstarted/#7-defining-a-project", 
            "text": "VC3, as a platform for cooperative scientific computing, allows you create\nprojects to share your allocations and virtual clusters with trusted members of\nyour group, laboratory, or collaboration. To start a new project, click\n\u201cProjects\u201d on the sidebar, then click \u201c+ New Project\u201d.   You may give your project an aribtrary name and choose initial project members and/or allocations.\nYou are, by default, member of all your projects.\nOnce finished, click \u201cCreate Project\u201d.   You should be returned to the Projects page, where you will be able to see all\nof your projects and memberships.\nYou can access the configuration of each project in the table by clicking on its name.\nThis way you can add or remove members and/or allocations to a given project.", 
            "title": "7. Defining a Project"
        }, 
        {
            "location": "/userguide/gettingstarted/#8-creating-a-cluster-template", 
            "text": "VC3 allows users to create \u201cCluster Templates\u201d that describe the components of\ntheir virtual cluster, including number of head nodes, worker nodes, etc. We\ncurrently support HTCondor and WorkQueue clusters with dynamic worker nodes,\nand fixed head nodes.  To define a new template, click the \u201cCluster Templates\u201d link on the left panel.\nYou\u2019ll be able to give your cluster a name, select framework, and number of\nworkers. Click \u201cDefine Cluster\u201d to finish creating the template.  You should be returned to the Cluster Templates page, where you will be able to see all of your templates.\nYou can access the configuration of each cluster template in the table by clicking on its name.", 
            "title": "8. Creating a Cluster Template"
        }, 
        {
            "location": "/userguide/gettingstarted/#9-environments", 
            "text": "An Enviroment is a collection of software packages to be included in a virtual cluster. \nClick on \"+New Environment\" to create new ones. \nFrom the \"PACKAGE LIST\" menu you can select one or more packages to be included in your new Enviroment.  From the second menu you can choose, if needed, the Operative System. If the chosen Operative System\nis not available natively, the system will attempt to provide it via containers.\nPlease, look into the  resources page  to check if the\nresource you are planning to use with this environment has container options available in the \n\"features\" section (E.g: singularity).   Once done, click on the \"Create New Enviroment\" button.\nYou should be returned to the Enviroments page, where you will be able to see all of your environments.\nYou can access the configuration of each environment in the table by clicking on its name.", 
            "title": "9. Environments"
        }, 
        {
            "location": "/userguide/gettingstarted/#10-launching-a-virtual-cluster", 
            "text": "Once you are part of a project, which has associated allocations,\nyou may launch a Virtual Cluster. In order to do so, click on\n\"New Virtual Cluster\", found in the Virtual Clusters tab from the left nav-menu.   From here, you are prompted to select the project that you would like to use in\norder to launch a Virtual Cluster. Once selected, press 'Next'.   Give your Virtual Cluster a name, select the cluster template, environment (optional) and allocation.\nYou may specify how long you would like your Virtual Cluster to run for. If not\nspecified, the expiration defaults to 72 hours. Which means that after 72 hours,\nyour Virtual Cluster will automatically begin to terminate itself.   Once launched, you will be redirected to your Virtual Cluster's detailed page,\nwhere you may track the status if your Virtual Cluster. The head node should take\na few moments to configure. Once complete, you will be prompted with instructions\non how to access the head node.", 
            "title": "10. Launching a Virtual Cluster"
        }, 
        {
            "location": "/resourceguide/newresource/", 
            "text": "Adding a resource to VC3\n\n\nThe \nVC3 project\n is searching for HPC resources to evaluate its closed-beta release.  If you would like to add an HPC resource to the VC3 platform for testing, please complete this \nform\n. You do not necessarily need to have an institutional affiliation with the HPC resource organization, but should have an allocation to run jobs there.  The VC3 user closed beta invitation signup is \nhere\n.\n\n\nRequirements\n\n\n\n\nCluster must have one of the following batch systems: Slurm, HTCondor, PBS, or Grid Engine.\n\n\nCompute nodes must have outbound IP connectivity.\n\n\nThe login node of the resource must be directly reachable via ssh (i.e., has a public IP) and not require MFA.", 
            "title": "Adding Your Resources"
        }, 
        {
            "location": "/resourceguide/newresource/#adding-a-resource-to-vc3", 
            "text": "The  VC3 project  is searching for HPC resources to evaluate its closed-beta release.  If you would like to add an HPC resource to the VC3 platform for testing, please complete this  form . You do not necessarily need to have an institutional affiliation with the HPC resource organization, but should have an allocation to run jobs there.  The VC3 user closed beta invitation signup is  here .", 
            "title": "Adding a resource to VC3"
        }, 
        {
            "location": "/resourceguide/newresource/#requirements", 
            "text": "Cluster must have one of the following batch systems: Slurm, HTCondor, PBS, or Grid Engine.  Compute nodes must have outbound IP connectivity.  The login node of the resource must be directly reachable via ssh (i.e., has a public IP) and not require MFA.", 
            "title": "Requirements"
        }, 
        {
            "location": "/devguide/builder/", 
            "text": "VC3 Builder\n\n\nvc3-builder\n - Deploy software environments in clusters without system administrator privileges.\n\n\nSYNOPSIS\n\n\n\n\nvc3-builder\n \n[options] --require package[:min_version[:max_version]] --require ... [-- command-and-args]\n\n\nDESCRIPTION\n\n\n\n\nThe \nvc3-builder\n is a tool to manage software stacks without administrator\npriviliges. Its primary application comes in deploying software dependencies in\ncloud, grid, and opportunistic computing, where deployment must be performed\ntogether with a batch job execution.\n\n\nvc3-builder\n is a self-contained program (including the repository of\ndependencies recipes). If desired, it can be compiled to a truly static binary\n(\nsee below\n).\n\n\nFrom the end-user perspective, \nvc3-builder\n is invoked as a command line\ntool which states the desired dependencies.  The builder will perform whatever\nwork is necessary to deliver those dependencies, then start a shell with the\nsoftware activated. For example, assume the original environment is a RHEL7, but we need to run the bioinformatics tool \nNCBI BLAST\n using RHEL6:\n\n\n$ cat /etc/redhat-release\nRed Hat Enterprise Linux Server release 7.4 (Maipo)\n$ ./vc3-builder --install ~/tmp/my-vc3 --require-os redhat6 --require ncbi-blast\nOS trying:         redhat6 os-native\nOS fail prereq:    redhat6 os-native\nOS trying:         redhat6 singularity\n..Plan:    ncbi-blast =\n [, ]\n..Try:     ncbi-blast =\n v2.2.28\n..Refining version: ncbi-blast v2.2.28 =\n [, ]\n..Success: ncbi-blast v2.2.28 =\n [, ]\nprocessing for ncbi-blast-v2.2.28\ndownloading 'ncbi-blast-2.2.28+-x64-linux.tar.gz' from http://download.virtualclusters.org/builder-files\npreparing 'ncbi-blast' for x86_64/redhat6.9\ndetails: /opt/vc3-root/x86_64/redhat6.9/ncbi-blast/v2.2.28/ncbi-blast-build-log\nsh-4.1$ cat /etc/redhat-release\nCentOS release 6.9 (Final)\nsh-4.1$ which blastn\n/opt/vc3-root/x86_64/redhat6.9/ncbi-blast/v2.2.28/bin/blastn\nsh-4.1$ exit\n$ ls -d ~/tmp/my-vc3\n/home/btovar/tmp/my-vc3\n\n\n\n\nIn the first stage, the builder verifies the operating system requirement.\nSince the native environment is not RHEL6, it tries to fulfill the requirement\nusing a container image. If the native environment would not support\ncontainers, the builder terminates indicating that the operating system\nrequirement cannot be fulfilled.\n\n\nIn the second stage, the builder checks if ncbi-blast is already installed.\nSince it is not, it downloads it and sets it up accordingly. As requested, all\nthe installation was done in \n/home/btovar/tmp/my-vc3\n, a directory that was\navailable as \n/opt/vc3-root\n inside the container.\n\n\nAs another example, the builder provides support for \ncvmfs\n:\n\n\n$ stat -t /cvmfs/cms.cern.ch\nstat: cannot stat '/cvmfs/cms.cern.ch': No such file or directory\n$ ./vc3-builder --require cvmfs\n./vc3-builder --require cvmfs\n..Plan:    cvmfs =\n [, ]\n..Try:     cvmfs =\n v2.4.0\n..Refining version: cvmfs v2.4.0 =\n [, ]\n....Plan:    cvmfs-parrot-libcvmfs =\n [v2.4.0, ]\n\n... etc ...\n\nsh-4.1$ stat -t /cvmfs/cms.cern.ch\n/cvmfs/cms.cern.ch 4096 9 41ed 0 0 1 256 1 0 1 1409299789 1409299789 1409299789 0 65336\n\n\n\n\nIn this case, the filesystem \ncvmfs\n is not provided natively and the builder\ntries to fulfill the requirement using the \nparrot virtual file system\n.\n\n\nThe builder installs dependencies as needed. For example, simply requiring\n\npython\n most likely will provide a python installation already in the system:\n\n\n$ ./vc3-builder --require python                             \n..Plan:    python =\n [, ]\n..Try:     python =\n v2.7.5\n..Refining version: python 2.7.5 =\n [, ]\n..Success: python v2.7.5 =\n [, ]\nprocessing for python-v2.7.5\nsh-4.2$ which python\n/bin/python\nsh-4.2$\n\n\n\n\nHowever, if we require the specific version:\n\n\n$ ./vc3-builder --require python:2.7.12\n..Plan:    python =\n [2.7.12, ]\n..Try:     python =\n v2.7.5\n..Incorrect version: v2.7.5 =\n [v2.7.12,]\n..Try:     python =\n v2.7.12\n..Refining version: python v2.7.12 =\n [v2.7.12, ]\n....Plan:    libffi =\n [v3.2.1, ]\n....Try:     libffi =\n v3.2.1\n....Refining version: libffi v3.2.1 =\n [v3.2.1, ]\n....Success: libffi v3.2.1 =\n [v3.2.1, ]\n\n... etc ...\n\nsh-4.2$ which python\n/home/btovar/vc3-root/x86_64/redhat7.4/python/v2.7.12/bin/python\n\n\n\n\nUse \n./vc3-builder --list\n to obtain the current list of packages available.\n\n\nHOME\n\n\n\n\nBy default, the HOME variable is to a directory created by the builder. This\ncan be changed using the \n--home\n option.\n\n\nMOUNTING FILESYSTEMS\n\n\n\n\nThe builder provides the \n--mount\n argument to optionally mount directories. It has two forms \n--mount /x\n and \n--mount /x:/y\n\n\n--mount /x\n\n\nIf executing in the native host environment, the builder simply ensured that the directory \n/x\n is accessible. If not, it terminates with an error.\n\n\nIf providing the environment with a container, the host environment path \n/x\n is mounted inside the container as \n/x\n.\n\n\n--mount /x:/y\n\n\nIf executing in the native host environment, and \n/x\n and \n/y\n are\ndifferent, the builder reports an error, otherwise it works as \n--mount /x\n.\n\n\nWhen executing inside a container, the host environment path \n/x\n is mounted\ninside the container as \n/y\n.\n\n\nEven when the host operating system fulfills the \n--require-os\n argument, a\ncontainer may still be used to fulfill a \n--mount\n requirement:\n\n\n$ ./vc3-builder --require-os redhat7 --mount /var/scratch/btovar:/madeuppath -- stat -t /madeuppath\nOS trying:         redhat7 os-native\nMount source '/var/scratch/btovar' and target '/madeuppath' are different.\nOS fail mounts:    redhat7 os-native\nOS trying:         redhat7 singularity\n/madeuppath 4096 8 41ed 196886 0 805 5111810 5 0 0 1520946165 1517595650 1517595650 0 4096\n$\n\n\n\n\nPARALLEL BUILD MODE\n\n\n\n\nIf a shared filesystem is available, the builder can be instructed to execute\nbuilds in parallel.  Only steps that can be executed concurrently, and for\nwhich their dependencies are already fulfilled are queued for execution.\n\n\nFor parallel build installations, use the \n--parallel\n option. It receives one\nargument, a directory to create the parallel build sandbox inside the builder's\nhome dir. For example, to build the bioinformatics pipeline \nmaker\n in parallel\nmode using SLURM:\n\n\n$ ./vc3-builder --require maker --install /scratch365/b/btovar/my-shared-dir --parallel my-parallel-build --parallel-mode slurm\n\n(... clipped build information...)\n\nParallel build mode complete. To run type:\n\nVC3_ROOT=/scratch365/b/btovar/my-shared-dir\nVC3_DB=/scratch365/b/btovar/my-shared-dir/my-parallel-build/recipes\n\n./vc3-builder --database ${VC3_DB} --install ${VC3_ROOT} --require maker\n\n\n\n\n\nIn addition to SLURM, other batch systems available are \ncondor\n, \nslurm\n,\n\nsge\n, \ntorque\n, \nmoab\n, \namazon\n, \nworkqueue\n and \nlocal\n. If a mode is not\nspecified, \nlocal\n is used.  \nlocal\n may also be used.\n\n\nRECIPES\n\n\n\n\nThe \nvc3-builder\n includes a repository of recipes. To list the packages available for the \n--require\n option, use:\n\n\n./vc3-builder --list\natlas-local-root-base-environment:v1.0\naugustus:v2.4\ncctools:v6.2.5\ncctools-unstable:v7.0.0\ncharm:v6.7.1\ncmake:auto\ncmake:v3.10.2\n... etc ...\n\n\n\n\nFor operating systems accepted by the \n--require-os\n option use:\n\n\n./vc3-builder --list=os    \ndebian9:auto\ndebian9:v9.2\nopensuse42:auto\nopensuse42:v42.3\nredhat6:auto\nredhat6:v6.9\nredhat7:auto\nredhat7:v7.4\nubuntu16:auto\nubuntu16:v16\n\n\n\n\nWhen a version appears as \nauto\n, it means that the builder knows how to\nrecognize that the correspoding requirement is already supplied by the host\nsystem.\n\n\nWRITING RECIPES\n\n\nThe builder can be provided with additional package recipes using the\n--database=\\\ncatalog> option. The option can be specified several times, with\nlatter package recipes overwriting previous ones.\n\n\nThe --database option accepts directory or file names. If a directory, it is\nsearched recursevely for files with the \n.json\n extension. Files are read in\nlexicographical order.\n\n\nA recipe catalog is a JSON encoded object, in which the keys of the object are\nthe names of the packages. Each package is a JSON object that, among other\nfields, specifies a list of versions of the package and a recipe to fulfill\nthat version.\n\n\nRecipes that provide packages\n\n\nAs an example, we will write the recipes for \nwget\n. First as a generic recipe,\nand then with different specific support that builder provides.\n\n\nA generic recipe:\n\n\n$ cat my-wget-recipe.json\n{\n    \nwget\n:{\n        \nversions\n:[\n            {\n                \nversion\n:\nv1.19.4\n,\n                \nsource\n:{\n                    \ntype\n:\ngeneric\n,\n                    \nfiles\n:[ \nwget-1.19.4.tar.gz\n ],\n                    \nrecipe\n:[\n                        \ntar xf wget-1.19.4.tar.gz\n,\n                        \n./configure --prefix=${VC3_PREFIX} --with-zlib --with-ssl=openssl --with-libssl-prefix=${VC3_ROOT_OPENSSL} --with-libuuid\n,\n                        \nmake\n,\n                        \nmake install\n\n                    ],\n                    \ndependencies\n:{\n                        \nzlib\n:[ \nv1.2\n ],\n                        \nopenssl\n:[ \nv1.0.2\n ],\n                        \nuuid\n:[ \nv1.0\n ],\n                        \nlibssh2\n:[ \nv1.8.0\n ]\n                    }\n                }\n            }\n        ],\n        \nenvironment-variables\n:[\n               {\n                \nname\n:\nPATH\n,\n                \nvalue\n:\nbin\n\n               }\n        ]\n    }\n}\n\n\n\n\nThe field \nversions\n inside the package definition is a list of JSON objects,\nwith each object providing the recipe for a version.\n\n\nThe files listed in \nfiles\n are automatically downloaded from the site pointed\nby the --repository option.  The source specification additionaly accepts the\n\nmirrors\n field, which is a list of alternative download location for \nfiles\n.\nMirrors are tried in order, finally falling back to the --repository option.\n\n\nThe lines in the \nrecipe\n field are executed one by one inside a shell.\n\n\nDependencies list the name of the package and a range of acceptable versions.\nIf only one version is provided, it is taken as a minimum acceptable version.\nDependencies can be specified per version, as in this case, or per package, in\nwhich case they are applied to all the versions.\n\n\nDuring the recipe execution, several environment variables are available. For\nexample, VC3_PREFIX, which points to the package installation directory. Each\npackage is installed into its own directory. Also, for each of the\ndependencies, a VC3_ROOT_dependency variable points to the dependency\ninstallation directory.\n\n\nWhen \nwget\n is set as a requirement, the value of \n$VC3_ROOT_WGET/bin\n is added\nto the \nPATH\n.\n\n\nA tarball recipe:\n\n\nWe can refine the recipe above by using the \ntarball\n source type, which automatically untars the first file listed in \nfiles\n:\n\n\n{\n    \nwget\n:{\n        \nversions\n:[\n            {\n                \nversion\n:\nv1.19.4\n,\n                \nsource\n:{\n                    \ntype\n:\ntarball\n,\n                    \nfiles\n:[ \nwget-1.19.4.tar.gz\n ],\n                    \nrecipe\n:[\n                        \n./configure --prefix=${VC3_PREFIX} --with-zlib --with-ssl=openssl --with-libssl-prefix=${VC3_ROOT_OPENSSL} --with-libuuid\n,\n                        \nmake\n,\n                        \nmake install\n\n                    ]\n                }\n            }\n        ],\n \n... etc ...\n\n  }\n}\n\n\n\n\n\nA configure recipe:\n\n\nFurther, we can do without the recipe using the \nconfigure\n type:\n\n\n{\n    \nwget\n:{\n        \nversions\n:[\n            {\n                \nversion\n:\nv1.19.4\n,\n                \nsource\n:{\n                    \ntype\n:\nconfigure\n,\n                    \nfiles\n:[ \nwget-1.19.4.tar.gz\n ],\n                    \noptions\n:\n--with-zlib --with-ssl=openssl --with-libssl-prefix=${VC3_ROOT_OPENSSL} --with-libuuid\n,\n                }\n            }\n        ],\n \n... etc ...\n\n  }\n}\n\n\n\n\nFor the \nconfigure\n type, there are also the \npreface\n and \npostface\n fields.\nThey are lists of shell commands (as \nrecipe\n), that execute before and after,\nrespectively, of the \nconfigure; make; make install\n step.\n\n\nAdding a binary distribution\n\n\n \nwget\n:{\n        \nversions\n:[\n            {\n               \nversion\n:\nv1.81\n,\n                \nsource\n:{\n                    \ntype\n:\nbinary\n,\n                    \nnative\n:\nx86_64\n,\n                    \nfiles\n:[\n                        \nwget-1.18-1.tar.gz\n\n                    ]\n                }\n            },\n            {\n                \nversion\n:\nv1.19.4\n,\n                \nsource\n:{\n                    \ntype\n:\nconfigure\n,\n \n... etc ...\n\n    }\n  }]\n}\n\n\n\n\n\nWe include the \nbinary\n version before the \nconfigure\n version as they are\ntried sequentially, and we would prefer not to build \nwget\n if it is not\nnecessary. The tarball provided includes a statically linked version of \nwget\n,\nand the architecture requirement is specified with the \nnative\n field.\n\n\nTarballs of binaries should have the file hierarchy: \ndir/{bin,etc}\n.\n\n\nAdding auto-detection:\n\n\n \nwget\n:{\n        \nversions\n:[\n            {\n                \nversion\n:\nauto\n,\n                \nsource\n:{\n                    \ntype\n:\nsystem\n,\n                    \nexecutable\n:\nwget\n\n                }\n            },\n            {\n               \nversion\n:\nv1.81\n,\n                \nsource\n:{\n                    \ntype\n:\nbinary\n,\n \n... etc ...\n\n    }\n  }]\n}\n\n\n\n\nIn this case, we simply provide the name of the executable to test, and the\nbuilder will try to get the version number out of the first line of the output\nfrom \nexecutable --version\n.\n\n\nIf an system executable does not provide version information in such manner,\n\nsource\n needs to provide an \nauto-version\n field that provides a recipe that\neventually prints to standard output a line such as:\n\n\nVC3_VERSION_SYSTEM: xxx.yyy.zzz\n\n\n\n\nFor example, in \nperl\n the version information is provided by the \n$^V\n\nvariable, and the \nauto-version\n field would look like:\n\n\n...\n\n        \nauto-version\n:[\n            \nperl -e  'print(\\\nVC3_VERSION_SYSTEM: $^V\\\\n\\\n);'\n\n        ],\n...\n\n\n\n\nNote that quotes and backslashes need to be escaped so that they are not\ninterpreted as part of the JSON structure.\n\n\nThe complete recipe\n\n\n{\n    \nwget\n:{\n        \ntags\n:[\ndata transfer tools\n],\n        \nshow-in-list\n:1,\n        \nversions\n:[\n            {\n                \nversion\n:\nauto\n,\n                \nsource\n:{\n                    \ntype\n:\nsystem\n,\n                    \nexecutable\n:\nwget\n\n                }\n            },\n            {\n                \nversion\n:\nv7.51\n,\n                \nsource\n:{\n                    \ntype\n:\nbinary\n,\n                    \nnative\n:\nx86_64\n,\n                    \nfiles\n:[\n                        \nwget-1.18-1.tar.gz\n\n                    ]\n                }\n            },\n            {\n                \nversion\n:\nv1.19.4\n,\n                \nsource\n:{\n                    \ntype\n:\nconfigure\n,\n                    \nfiles\n:[ \nwget-1.19.4.tar.gz\n ],\n                    \noptions\n:\n--with-zlib --with-ssl=openssl --with-libssl-prefix=${VC3_ROOT_OPENSSL} --with-libuuid\n,\n                },\n                \ndependencies\n:{\n                    \nzlib\n:[\n                        \nv1.2\n\n                    ],\n                    \nopenssl\n:[\n                        \nv1.0.2\n\n                    ],\n                    \nuuid\n:[\n                        \nv1.0\n\n                    ],\n                    \nlibssh2\n:[\n                        \nv1.8.0\n\n                    ]\n                }\n            }\n        ],\n        \nenvironment-autovars\n:[\n            \nPATH\n\n        ]\n    },\n}\n\n\n\n\nWe made three changes:\n\n\n\n\nAdded the \ntags\n field to classify the package. Listing of packages by tags is available with the \n--list=section\n option.\n\n\nAdded \nshow-in-list\n field, which allows the package to be displayed by \n--list\n.\n\n\nSince adding \n${VC3_ROOT_package}/bin\n to the \nPATH\n is a common operation,\nthe builder provides the \"environment-autovars\" field, which automatically\nconstructs common patterns for the variables \nPATH\n, \nLD_LIBRARY_PATH\n,\n\nPKG_CONFIG_PATH\n, \nLIBRARY_PATH\n, \nC_INCLUDE_PATH\n, \nCPLUS_INCLUDE_PATH\n, and\n\nPERL5LIB\n.  Support for \nPYTHONPATH\n is not provided, as there is not an easy\nway to handle concurrent \npython2\n and \npython3\n installations.\n\n\n\n\nRecipes that provide environments\n\n\nEnvironment prologues\n\n\nIt is sometimes required to run a command to complete setting the environment.\nFor example, a script containing evinronment variables may need to be sourced\nbefore execution. For such cases, the \nprologue\n field can be used. The\nfollowing is an example for setting the \nOSG\noasis\n\nenvironment:\n\n\n...\n\n\noasis-environment\n:{\n        \nversions\n:[\n            {\n                \nversion\n:\nv1.0\n,\n                \ntype\n:\ngeneric\n,\n                \nprologue\n: [\n                    \nsource /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash\n\n                ],\n                \ndependencies\n:{\n                    \ncvmfs\n:[\n                        \nv2.0\n\n                    ]\n                }\n            }\n        ]\n    },\n\n...\n\n\n\n\n\nThe lines in the \nprologue\n field are executed for every new shell executed\ninside the builder environment. Note that in this particular case there was no\nneed to provide a \nsource\n field.\n\n\nEnvironment wrappers\n\n\nA wrapper is any program that executes the payload of the builder. In the usual\ncase, there is no wrapper, and the builder simply executes its payload using\n\n/bin/sh\n. In the \nintroductory examples\n, we showed the builder\naccessing a filesystem (cvmfs) that was not present in the host system. This\nwas done by using the \nparrot virtual file\nsystem\n as a wrapper as follows:\n\n\n \nparrot-wrapper\n:{\n        \nversions\n:[\n            {\n                \nversion\n:\nv6.0.0\n,\n                \nwrapper\n:[\n                    \nparrot_run\n, \n--dynamic-mounts\n, \n{}\n\n                ],\n                \ndependencies\n:{\n                    \ncctools\n:[\n                        \nv6.0.0\n\n                    ]\n                }\n            }\n        ]\n    }\n\n\n\n\nThe \nparrot_run\n executable is provided through the \ncctools\n dependency. The\nwrapper itself is specified as a list of arguments in the \nwrapper\n field. The\npayload is substituted in place of \n{}\n. If several wrappers are required, they\nnest inner-to-outer as they appear as arguments to \n--require\n in the command\nline.\n\n\nOperating system recipes\n\n\nOperating systems recipes are similar to package recipes, but they are labeled\nwith the \noperating-system\n field. An operating system requirement is specified\nwith the \n--require-os\n option.\n\n\nHere we include an example for Red Hat 7:\n\n\n...\n\n    \nredhat7\n:{\n        \ntags\n:[\noperating systems\n],\n        \nshow-in-list\n:1,\n        \noperating-system\n:1,\n        \nversions\n:[\n            {\n                \nversion\n:\nauto\n,\n                \nsource\n:{\n                    \ntype\n:\nos-native\n,\n                    \nnative\n:\nx86_64/redhat7\n\n                }\n            },\n            {\n                \nversion\n:\n7.4\n,\n                \nsource\n:{\n                    \ntype\n:\nsingularity\n,\n                    \nimage\n:\nSingularity.vc3.x86_64-centos7.img\n\n                }\n            }\n        ]\n    },\n\n...\n\n\n\n\n\nFor the \nos-native\n type, the \nnative\n field specifies the target system. It is\nof the form \narchitecture/distribution\n. Use \n./vc3-builder --list=os\n for a\nlist of known distributions.\n\n\nIn the \nsingularity\n type, the image file provided is downloaded from\n\nrepository\\\n/images/singularity\n, where repository is specified by the\n\n--repository\n option. If the image is not file, but starts with \ndocker://\n or\n\nshub://\n, it is downloaded from the corresponding image repository.\n\n\nRecipes bits-and-pieces\n\n\nThere are three more fields a version specification accepts:\n\n\n\n\nprerequisites\n: A list of shell commands which need to succed for the\nversion to be included in the build plan. Useful to check if some file is\npresent, for example.\n\n\nphony\n: In normal operation, the builder executes the recipe of a source\nonly once. If \nphony\n is set to \n1\n, the recipe is executed every time the\npackage is required.\n\n\nlocal\n: Only relevant for the \nparallel build mode\n.\nIndicates that the recipe should be executed locally, and not in a remote\ncomputational node. It is useful when the recipe takes very little time\ncompared to scheduling it for parallel execution.\n\n\n\n\nAdding recipes to the vc3-builder executable\n\n\n\n\nFirst, clone the \nvc3-builder\n repository:\n\n\ngit clone https://github.com/vc3-project/vc3-builder.git\ncd vc3-builder\n\n\n\n\nSecond, write any recipe files you want included in the \nrecipes\n directory.\nRecipe files names may not contain spaces.  Recipes may be organized in\ndirectories, that are read recursevely. Files are read in lexicographical\norder, with later recipe definitions overwriting previous ones if package names\nare repeated.\n\n\nFinally, use \nmake\n to construct the new \nvc3-builder\n with your recipes included:\n\n\nmake clean vc3-builder\n\n\n\n\nCompiling the Builder as a static binary\n\n\n\n\ngit clone https://github.com/vc3-project/vc3-builder.git\ncd vc3-builder\nmake vc3-builder-static\n\n\n\n\nThe static version will be available at \nvc3-builder-static\n.\nThe steps above set a local \nmusl-libc\n installation that compile \nvc3-builder\n into a \nstatic perl\n interpreter.\n\n\nReference\n\n\n\n\nBenjamin Tovar, Nicholas Hazekamp, Nathaniel Kremer-Herman, and Douglas Thain.\n\nAutomatic Dependency Management for Scientific Applications on Clusters,\n\nIEEE International Conference on Cloud Engineering (IC2E), April, 2018.", 
            "title": "VC3 Builder"
        }, 
        {
            "location": "/devguide/builder/#vc3-builder", 
            "text": "vc3-builder  - Deploy software environments in clusters without system administrator privileges.", 
            "title": "VC3 Builder"
        }, 
        {
            "location": "/devguide/builder/#synopsis", 
            "text": "vc3-builder   [options] --require package[:min_version[:max_version]] --require ... [-- command-and-args]", 
            "title": "SYNOPSIS"
        }, 
        {
            "location": "/devguide/builder/#description", 
            "text": "The  vc3-builder  is a tool to manage software stacks without administrator\npriviliges. Its primary application comes in deploying software dependencies in\ncloud, grid, and opportunistic computing, where deployment must be performed\ntogether with a batch job execution.  vc3-builder  is a self-contained program (including the repository of\ndependencies recipes). If desired, it can be compiled to a truly static binary\n( see below ).  From the end-user perspective,  vc3-builder  is invoked as a command line\ntool which states the desired dependencies.  The builder will perform whatever\nwork is necessary to deliver those dependencies, then start a shell with the\nsoftware activated. For example, assume the original environment is a RHEL7, but we need to run the bioinformatics tool  NCBI BLAST  using RHEL6:  $ cat /etc/redhat-release\nRed Hat Enterprise Linux Server release 7.4 (Maipo)\n$ ./vc3-builder --install ~/tmp/my-vc3 --require-os redhat6 --require ncbi-blast\nOS trying:         redhat6 os-native\nOS fail prereq:    redhat6 os-native\nOS trying:         redhat6 singularity\n..Plan:    ncbi-blast =  [, ]\n..Try:     ncbi-blast =  v2.2.28\n..Refining version: ncbi-blast v2.2.28 =  [, ]\n..Success: ncbi-blast v2.2.28 =  [, ]\nprocessing for ncbi-blast-v2.2.28\ndownloading 'ncbi-blast-2.2.28+-x64-linux.tar.gz' from http://download.virtualclusters.org/builder-files\npreparing 'ncbi-blast' for x86_64/redhat6.9\ndetails: /opt/vc3-root/x86_64/redhat6.9/ncbi-blast/v2.2.28/ncbi-blast-build-log\nsh-4.1$ cat /etc/redhat-release\nCentOS release 6.9 (Final)\nsh-4.1$ which blastn\n/opt/vc3-root/x86_64/redhat6.9/ncbi-blast/v2.2.28/bin/blastn\nsh-4.1$ exit\n$ ls -d ~/tmp/my-vc3\n/home/btovar/tmp/my-vc3  In the first stage, the builder verifies the operating system requirement.\nSince the native environment is not RHEL6, it tries to fulfill the requirement\nusing a container image. If the native environment would not support\ncontainers, the builder terminates indicating that the operating system\nrequirement cannot be fulfilled.  In the second stage, the builder checks if ncbi-blast is already installed.\nSince it is not, it downloads it and sets it up accordingly. As requested, all\nthe installation was done in  /home/btovar/tmp/my-vc3 , a directory that was\navailable as  /opt/vc3-root  inside the container.  As another example, the builder provides support for  cvmfs :  $ stat -t /cvmfs/cms.cern.ch\nstat: cannot stat '/cvmfs/cms.cern.ch': No such file or directory\n$ ./vc3-builder --require cvmfs\n./vc3-builder --require cvmfs\n..Plan:    cvmfs =  [, ]\n..Try:     cvmfs =  v2.4.0\n..Refining version: cvmfs v2.4.0 =  [, ]\n....Plan:    cvmfs-parrot-libcvmfs =  [v2.4.0, ]\n\n... etc ...\n\nsh-4.1$ stat -t /cvmfs/cms.cern.ch\n/cvmfs/cms.cern.ch 4096 9 41ed 0 0 1 256 1 0 1 1409299789 1409299789 1409299789 0 65336  In this case, the filesystem  cvmfs  is not provided natively and the builder\ntries to fulfill the requirement using the  parrot virtual file system .  The builder installs dependencies as needed. For example, simply requiring python  most likely will provide a python installation already in the system:  $ ./vc3-builder --require python                             \n..Plan:    python =  [, ]\n..Try:     python =  v2.7.5\n..Refining version: python 2.7.5 =  [, ]\n..Success: python v2.7.5 =  [, ]\nprocessing for python-v2.7.5\nsh-4.2$ which python\n/bin/python\nsh-4.2$  However, if we require the specific version:  $ ./vc3-builder --require python:2.7.12\n..Plan:    python =  [2.7.12, ]\n..Try:     python =  v2.7.5\n..Incorrect version: v2.7.5 =  [v2.7.12,]\n..Try:     python =  v2.7.12\n..Refining version: python v2.7.12 =  [v2.7.12, ]\n....Plan:    libffi =  [v3.2.1, ]\n....Try:     libffi =  v3.2.1\n....Refining version: libffi v3.2.1 =  [v3.2.1, ]\n....Success: libffi v3.2.1 =  [v3.2.1, ]\n\n... etc ...\n\nsh-4.2$ which python\n/home/btovar/vc3-root/x86_64/redhat7.4/python/v2.7.12/bin/python  Use  ./vc3-builder --list  to obtain the current list of packages available.", 
            "title": "DESCRIPTION"
        }, 
        {
            "location": "/devguide/builder/#home", 
            "text": "By default, the HOME variable is to a directory created by the builder. This\ncan be changed using the  --home  option.", 
            "title": "HOME"
        }, 
        {
            "location": "/devguide/builder/#mounting-filesystems", 
            "text": "The builder provides the  --mount  argument to optionally mount directories. It has two forms  --mount /x  and  --mount /x:/y", 
            "title": "MOUNTING FILESYSTEMS"
        }, 
        {
            "location": "/devguide/builder/#-mount-x", 
            "text": "If executing in the native host environment, the builder simply ensured that the directory  /x  is accessible. If not, it terminates with an error.  If providing the environment with a container, the host environment path  /x  is mounted inside the container as  /x .", 
            "title": "--mount /x"
        }, 
        {
            "location": "/devguide/builder/#-mount-xy", 
            "text": "If executing in the native host environment, and  /x  and  /y  are\ndifferent, the builder reports an error, otherwise it works as  --mount /x .  When executing inside a container, the host environment path  /x  is mounted\ninside the container as  /y .  Even when the host operating system fulfills the  --require-os  argument, a\ncontainer may still be used to fulfill a  --mount  requirement:  $ ./vc3-builder --require-os redhat7 --mount /var/scratch/btovar:/madeuppath -- stat -t /madeuppath\nOS trying:         redhat7 os-native\nMount source '/var/scratch/btovar' and target '/madeuppath' are different.\nOS fail mounts:    redhat7 os-native\nOS trying:         redhat7 singularity\n/madeuppath 4096 8 41ed 196886 0 805 5111810 5 0 0 1520946165 1517595650 1517595650 0 4096\n$", 
            "title": "--mount /x:/y"
        }, 
        {
            "location": "/devguide/builder/#parallel-build-mode", 
            "text": "If a shared filesystem is available, the builder can be instructed to execute\nbuilds in parallel.  Only steps that can be executed concurrently, and for\nwhich their dependencies are already fulfilled are queued for execution.  For parallel build installations, use the  --parallel  option. It receives one\nargument, a directory to create the parallel build sandbox inside the builder's\nhome dir. For example, to build the bioinformatics pipeline  maker  in parallel\nmode using SLURM:  $ ./vc3-builder --require maker --install /scratch365/b/btovar/my-shared-dir --parallel my-parallel-build --parallel-mode slurm\n\n(... clipped build information...)\n\nParallel build mode complete. To run type:\n\nVC3_ROOT=/scratch365/b/btovar/my-shared-dir\nVC3_DB=/scratch365/b/btovar/my-shared-dir/my-parallel-build/recipes\n\n./vc3-builder --database ${VC3_DB} --install ${VC3_ROOT} --require maker  In addition to SLURM, other batch systems available are  condor ,  slurm , sge ,  torque ,  moab ,  amazon ,  workqueue  and  local . If a mode is not\nspecified,  local  is used.   local  may also be used.", 
            "title": "PARALLEL BUILD MODE"
        }, 
        {
            "location": "/devguide/builder/#recipes", 
            "text": "The  vc3-builder  includes a repository of recipes. To list the packages available for the  --require  option, use:  ./vc3-builder --list\natlas-local-root-base-environment:v1.0\naugustus:v2.4\ncctools:v6.2.5\ncctools-unstable:v7.0.0\ncharm:v6.7.1\ncmake:auto\ncmake:v3.10.2\n... etc ...  For operating systems accepted by the  --require-os  option use:  ./vc3-builder --list=os    \ndebian9:auto\ndebian9:v9.2\nopensuse42:auto\nopensuse42:v42.3\nredhat6:auto\nredhat6:v6.9\nredhat7:auto\nredhat7:v7.4\nubuntu16:auto\nubuntu16:v16  When a version appears as  auto , it means that the builder knows how to\nrecognize that the correspoding requirement is already supplied by the host\nsystem.", 
            "title": "RECIPES"
        }, 
        {
            "location": "/devguide/builder/#writing-recipes", 
            "text": "The builder can be provided with additional package recipes using the\n--database=\\ catalog> option. The option can be specified several times, with\nlatter package recipes overwriting previous ones.  The --database option accepts directory or file names. If a directory, it is\nsearched recursevely for files with the  .json  extension. Files are read in\nlexicographical order.  A recipe catalog is a JSON encoded object, in which the keys of the object are\nthe names of the packages. Each package is a JSON object that, among other\nfields, specifies a list of versions of the package and a recipe to fulfill\nthat version.", 
            "title": "WRITING RECIPES"
        }, 
        {
            "location": "/devguide/builder/#recipes-that-provide-packages", 
            "text": "As an example, we will write the recipes for  wget . First as a generic recipe,\nand then with different specific support that builder provides.", 
            "title": "Recipes that provide packages"
        }, 
        {
            "location": "/devguide/builder/#a-generic-recipe", 
            "text": "$ cat my-wget-recipe.json\n{\n     wget :{\n         versions :[\n            {\n                 version : v1.19.4 ,\n                 source :{\n                     type : generic ,\n                     files :[  wget-1.19.4.tar.gz  ],\n                     recipe :[\n                         tar xf wget-1.19.4.tar.gz ,\n                         ./configure --prefix=${VC3_PREFIX} --with-zlib --with-ssl=openssl --with-libssl-prefix=${VC3_ROOT_OPENSSL} --with-libuuid ,\n                         make ,\n                         make install \n                    ],\n                     dependencies :{\n                         zlib :[  v1.2  ],\n                         openssl :[  v1.0.2  ],\n                         uuid :[  v1.0  ],\n                         libssh2 :[  v1.8.0  ]\n                    }\n                }\n            }\n        ],\n         environment-variables :[\n               {\n                 name : PATH ,\n                 value : bin \n               }\n        ]\n    }\n}  The field  versions  inside the package definition is a list of JSON objects,\nwith each object providing the recipe for a version.  The files listed in  files  are automatically downloaded from the site pointed\nby the --repository option.  The source specification additionaly accepts the mirrors  field, which is a list of alternative download location for  files .\nMirrors are tried in order, finally falling back to the --repository option.  The lines in the  recipe  field are executed one by one inside a shell.  Dependencies list the name of the package and a range of acceptable versions.\nIf only one version is provided, it is taken as a minimum acceptable version.\nDependencies can be specified per version, as in this case, or per package, in\nwhich case they are applied to all the versions.  During the recipe execution, several environment variables are available. For\nexample, VC3_PREFIX, which points to the package installation directory. Each\npackage is installed into its own directory. Also, for each of the\ndependencies, a VC3_ROOT_dependency variable points to the dependency\ninstallation directory.  When  wget  is set as a requirement, the value of  $VC3_ROOT_WGET/bin  is added\nto the  PATH .", 
            "title": "A generic recipe:"
        }, 
        {
            "location": "/devguide/builder/#a-tarball-recipe", 
            "text": "We can refine the recipe above by using the  tarball  source type, which automatically untars the first file listed in  files :  {\n     wget :{\n         versions :[\n            {\n                 version : v1.19.4 ,\n                 source :{\n                     type : tarball ,\n                     files :[  wget-1.19.4.tar.gz  ],\n                     recipe :[\n                         ./configure --prefix=${VC3_PREFIX} --with-zlib --with-ssl=openssl --with-libssl-prefix=${VC3_ROOT_OPENSSL} --with-libuuid ,\n                         make ,\n                         make install \n                    ]\n                }\n            }\n        ],\n  ... etc ... \n  }\n}", 
            "title": "A tarball recipe:"
        }, 
        {
            "location": "/devguide/builder/#a-configure-recipe", 
            "text": "Further, we can do without the recipe using the  configure  type:  {\n     wget :{\n         versions :[\n            {\n                 version : v1.19.4 ,\n                 source :{\n                     type : configure ,\n                     files :[  wget-1.19.4.tar.gz  ],\n                     options : --with-zlib --with-ssl=openssl --with-libssl-prefix=${VC3_ROOT_OPENSSL} --with-libuuid ,\n                }\n            }\n        ],\n  ... etc ... \n  }\n}  For the  configure  type, there are also the  preface  and  postface  fields.\nThey are lists of shell commands (as  recipe ), that execute before and after,\nrespectively, of the  configure; make; make install  step.", 
            "title": "A configure recipe:"
        }, 
        {
            "location": "/devguide/builder/#adding-a-binary-distribution", 
            "text": "wget :{\n         versions :[\n            {\n                version : v1.81 ,\n                 source :{\n                     type : binary ,\n                     native : x86_64 ,\n                     files :[\n                         wget-1.18-1.tar.gz \n                    ]\n                }\n            },\n            {\n                 version : v1.19.4 ,\n                 source :{\n                     type : configure ,\n  ... etc ... \n    }\n  }]\n}  We include the  binary  version before the  configure  version as they are\ntried sequentially, and we would prefer not to build  wget  if it is not\nnecessary. The tarball provided includes a statically linked version of  wget ,\nand the architecture requirement is specified with the  native  field.  Tarballs of binaries should have the file hierarchy:  dir/{bin,etc} .", 
            "title": "Adding a binary distribution"
        }, 
        {
            "location": "/devguide/builder/#adding-auto-detection", 
            "text": "wget :{\n         versions :[\n            {\n                 version : auto ,\n                 source :{\n                     type : system ,\n                     executable : wget \n                }\n            },\n            {\n                version : v1.81 ,\n                 source :{\n                     type : binary ,\n  ... etc ... \n    }\n  }]\n}  In this case, we simply provide the name of the executable to test, and the\nbuilder will try to get the version number out of the first line of the output\nfrom  executable --version .  If an system executable does not provide version information in such manner, source  needs to provide an  auto-version  field that provides a recipe that\neventually prints to standard output a line such as:  VC3_VERSION_SYSTEM: xxx.yyy.zzz  For example, in  perl  the version information is provided by the  $^V \nvariable, and the  auto-version  field would look like:  ...\n\n         auto-version :[\n             perl -e  'print(\\ VC3_VERSION_SYSTEM: $^V\\\\n\\ );' \n        ],\n...  Note that quotes and backslashes need to be escaped so that they are not\ninterpreted as part of the JSON structure.", 
            "title": "Adding auto-detection:"
        }, 
        {
            "location": "/devguide/builder/#the-complete-recipe", 
            "text": "{\n     wget :{\n         tags :[ data transfer tools ],\n         show-in-list :1,\n         versions :[\n            {\n                 version : auto ,\n                 source :{\n                     type : system ,\n                     executable : wget \n                }\n            },\n            {\n                 version : v7.51 ,\n                 source :{\n                     type : binary ,\n                     native : x86_64 ,\n                     files :[\n                         wget-1.18-1.tar.gz \n                    ]\n                }\n            },\n            {\n                 version : v1.19.4 ,\n                 source :{\n                     type : configure ,\n                     files :[  wget-1.19.4.tar.gz  ],\n                     options : --with-zlib --with-ssl=openssl --with-libssl-prefix=${VC3_ROOT_OPENSSL} --with-libuuid ,\n                },\n                 dependencies :{\n                     zlib :[\n                         v1.2 \n                    ],\n                     openssl :[\n                         v1.0.2 \n                    ],\n                     uuid :[\n                         v1.0 \n                    ],\n                     libssh2 :[\n                         v1.8.0 \n                    ]\n                }\n            }\n        ],\n         environment-autovars :[\n             PATH \n        ]\n    },\n}  We made three changes:   Added the  tags  field to classify the package. Listing of packages by tags is available with the  --list=section  option.  Added  show-in-list  field, which allows the package to be displayed by  --list .  Since adding  ${VC3_ROOT_package}/bin  to the  PATH  is a common operation,\nthe builder provides the \"environment-autovars\" field, which automatically\nconstructs common patterns for the variables  PATH ,  LD_LIBRARY_PATH , PKG_CONFIG_PATH ,  LIBRARY_PATH ,  C_INCLUDE_PATH ,  CPLUS_INCLUDE_PATH , and PERL5LIB .  Support for  PYTHONPATH  is not provided, as there is not an easy\nway to handle concurrent  python2  and  python3  installations.", 
            "title": "The complete recipe"
        }, 
        {
            "location": "/devguide/builder/#recipes-that-provide-environments", 
            "text": "", 
            "title": "Recipes that provide environments"
        }, 
        {
            "location": "/devguide/builder/#environment-prologues", 
            "text": "It is sometimes required to run a command to complete setting the environment.\nFor example, a script containing evinronment variables may need to be sourced\nbefore execution. For such cases, the  prologue  field can be used. The\nfollowing is an example for setting the  OSG\noasis \nenvironment:  ...  oasis-environment :{\n         versions :[\n            {\n                 version : v1.0 ,\n                 type : generic ,\n                 prologue : [\n                     source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash \n                ],\n                 dependencies :{\n                     cvmfs :[\n                         v2.0 \n                    ]\n                }\n            }\n        ]\n    }, ...   The lines in the  prologue  field are executed for every new shell executed\ninside the builder environment. Note that in this particular case there was no\nneed to provide a  source  field.", 
            "title": "Environment prologues"
        }, 
        {
            "location": "/devguide/builder/#environment-wrappers", 
            "text": "A wrapper is any program that executes the payload of the builder. In the usual\ncase, there is no wrapper, and the builder simply executes its payload using /bin/sh . In the  introductory examples , we showed the builder\naccessing a filesystem (cvmfs) that was not present in the host system. This\nwas done by using the  parrot virtual file\nsystem  as a wrapper as follows:    parrot-wrapper :{\n         versions :[\n            {\n                 version : v6.0.0 ,\n                 wrapper :[\n                     parrot_run ,  --dynamic-mounts ,  {} \n                ],\n                 dependencies :{\n                     cctools :[\n                         v6.0.0 \n                    ]\n                }\n            }\n        ]\n    }  The  parrot_run  executable is provided through the  cctools  dependency. The\nwrapper itself is specified as a list of arguments in the  wrapper  field. The\npayload is substituted in place of  {} . If several wrappers are required, they\nnest inner-to-outer as they appear as arguments to  --require  in the command\nline.", 
            "title": "Environment wrappers"
        }, 
        {
            "location": "/devguide/builder/#operating-system-recipes", 
            "text": "Operating systems recipes are similar to package recipes, but they are labeled\nwith the  operating-system  field. An operating system requirement is specified\nwith the  --require-os  option.  Here we include an example for Red Hat 7:  ... \n     redhat7 :{\n         tags :[ operating systems ],\n         show-in-list :1,\n         operating-system :1,\n         versions :[\n            {\n                 version : auto ,\n                 source :{\n                     type : os-native ,\n                     native : x86_64/redhat7 \n                }\n            },\n            {\n                 version : 7.4 ,\n                 source :{\n                     type : singularity ,\n                     image : Singularity.vc3.x86_64-centos7.img \n                }\n            }\n        ]\n    }, ...   For the  os-native  type, the  native  field specifies the target system. It is\nof the form  architecture/distribution . Use  ./vc3-builder --list=os  for a\nlist of known distributions.  In the  singularity  type, the image file provided is downloaded from repository\\ /images/singularity , where repository is specified by the --repository  option. If the image is not file, but starts with  docker://  or shub:// , it is downloaded from the corresponding image repository.", 
            "title": "Operating system recipes"
        }, 
        {
            "location": "/devguide/builder/#recipes-bits-and-pieces", 
            "text": "There are three more fields a version specification accepts:   prerequisites : A list of shell commands which need to succed for the\nversion to be included in the build plan. Useful to check if some file is\npresent, for example.  phony : In normal operation, the builder executes the recipe of a source\nonly once. If  phony  is set to  1 , the recipe is executed every time the\npackage is required.  local : Only relevant for the  parallel build mode .\nIndicates that the recipe should be executed locally, and not in a remote\ncomputational node. It is useful when the recipe takes very little time\ncompared to scheduling it for parallel execution.", 
            "title": "Recipes bits-and-pieces"
        }, 
        {
            "location": "/devguide/builder/#adding-recipes-to-the-vc3-builder-executable", 
            "text": "First, clone the  vc3-builder  repository:  git clone https://github.com/vc3-project/vc3-builder.git\ncd vc3-builder  Second, write any recipe files you want included in the  recipes  directory.\nRecipe files names may not contain spaces.  Recipes may be organized in\ndirectories, that are read recursevely. Files are read in lexicographical\norder, with later recipe definitions overwriting previous ones if package names\nare repeated.  Finally, use  make  to construct the new  vc3-builder  with your recipes included:  make clean vc3-builder", 
            "title": "Adding recipes to the vc3-builder executable"
        }, 
        {
            "location": "/devguide/builder/#compiling-the-builder-as-a-static-binary", 
            "text": "git clone https://github.com/vc3-project/vc3-builder.git\ncd vc3-builder\nmake vc3-builder-static  The static version will be available at  vc3-builder-static .\nThe steps above set a local  musl-libc  installation that compile  vc3-builder  into a  static perl  interpreter.", 
            "title": "Compiling the Builder as a static binary"
        }, 
        {
            "location": "/devguide/builder/#reference", 
            "text": "Benjamin Tovar, Nicholas Hazekamp, Nathaniel Kremer-Herman, and Douglas Thain. Automatic Dependency Management for Scientific Applications on Clusters, \nIEEE International Conference on Cloud Engineering (IC2E), April, 2018.", 
            "title": "Reference"
        }, 
        {
            "location": "/devguide/deployment-guide/", 
            "text": "Prerequisites for all services\n\n\nAdding keys\n\n\nWhen a piece of static infrastructure is initialized on OpenStack or AWS, it will only contain \nyour\n public key at first. You'll need to add the public keys for other developers as well.\n\n\nvi .ssh/authorized_keys\n\n\n\n\nThe CentOS account should, by default, have \nsudo\n privileges. \n\n\n\n\nInstalling the repos\n\n\nOn all pieces of the static infrastructure, you will need to install the VC3 package repository. Add the following contents to \n/etc/yum.repos.d/vc3.repo\n:\n\n\n[vc3-x86_64]\nname=VC3 x86_64\nbaseurl=http://build.virtualclusters.org/production/x86_64\ngpgcheck=0\n[vc3-noarch]\nname=VC3 noarch\nbaseurl=http://build.virtualclusters.org/production/noarch\ngpgcheck=0\n\n\n\n\n\n\nBootstrapping authentication on the Master\n\n\nInstalling Credible and setting up certificates\n\n\nCredible will be used to issue certificates to all VC3 components for authentication. We will only set it up on the Master, and then copy certificates and keys to other pieces of the infrastructure. \n\n\nInstall credible from the repos:\n\n\nyum install credible\n\n\n\n\nAnd update \n/etc/credible/credible.conf\n as appropriate:\n\n\n[credible]\nstorageplugin = Memory\nvardir = /var/credible\n\n[credible-ssh]\nkeytype = rsa\nbitlength = 4096\n\n[credible-ssca]\nroottemplate=/etc/credible/openssl.cnf.root.template\nintermediatetemplate=/etc/credible/openssl.cnf.intermediate.template\nvardir = /var/credible\nbitlength = 4096\n\n# Test defaults.\nsscaname = VC3\ncountry = US\nlocality = Upton\nstate = NY\norganization = BNL\norgunit = SDCC\nemail = jhover@bnl.gov\n\n\n\n\nIssuing certificates\n\n\nRetrieve the CA Chain and generate a host certificate and key for the Master:\n\n\ncredible -c /etc/credible/credible.conf hostcert master-test.virtualclusters.org \n /etc/pki/tls/certs/hostcert.pem\ncredible -c /etc/credible/credible.conf hostkey master-test.virtualclusters.org \n /etc/pki/tls/private/hostkey.pem\ncredible -c /etc/credible/credible.conf certchain \n /etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\n\n\n\n\n\n\nVC3 Infoservice\n\n\nPrerequisites\n\n\nAs with the Master, you will need to install the VC3 repo and any public keys. \n\n\nInstalling the Infoservice\n\n\nAssuming you have already configured the VC3 repos, install the following:\n\n\nyum install epel-release -y\nyum install vc3-infoservice pluginmanager python-pip openssl -y\npip install pyOpenSSL CherryPy==11.0.0\n\n\n\n\nAnd edit config at \n/etc/vc3/vc3-infoservice.conf\n:\n\n\n[DEFAULT]\nloglevel = debug\n\n[netcomm]\nchainfile=/etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\ncertfile=/etc/pki/tls/certs/hostcert.pem\nkeyfile=/etc/pki/tls/private/hostkey.pem\n\nsslmodule=pyopenssl\nhttpport=20333\nhttpsport=20334\n\n[persistence]\nplugin = DiskDump\n\n[plugin-diskdump]\nfilename=/tmp/infoservice.diskdump\n\n\n\n\nOn the \nMaster\n host, you will need to issue certificates for the Infoservice. Copy and paste into the appropriate files:\n\n\n(master)# credible -c /etc/credible/credible.conf hostcert info-test.virtualclusters.org\n(infoservice)# vi /etc/pki/tls/certs/hostcert.pem\n(master)# credible -c /etc/credible/credible.conf hostkey info-test.virtualclusters.org\n(infoservice)# vi /etc/pki/tls/private/hostkey.pem\n(master)# credible -c /etc/credible/credible.conf certchain\n(infoservice)# /etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\n\n\n\n\nStarting the Infoservice\n\n\nDue to a bug (CORE-261), we need to create \n/var/log/vc3\n\n\nmkdir -p /var/log/vc3\n\n\n\n\nFor now, we'll use the sysv-init style startup scripts:\n\n\n/etc/init.d/vc3-infoservice.init start\n\n\n\n\nIf it's running, you should see the following after \n/etc/init.d/vc3-infoservice.init status\n:\n\n\n\u25cf vc3-infoservice.init.service - LSB: start and stop vc3-info-service\n   Loaded: loaded (/etc/rc.d/init.d/vc3-infoservice.init; bad; vendor preset: disabled)\n   Active: active (running) since Fri 2017-09-08 16:31:13 UTC; 36s ago\n\n\n\n\nCheck the logs for any ERROR statements:\n\n\ngrep \nERROR\n /var/log/vc3/*log || echo \nEverything OK\n\n\n\n\n\n\n\nVC3 Master\n\n\nInstalling the Master\n\n\nThe Master depends on the vc3-client and vc3-infoservice packages for the client APIs. We also need ansible and the VC3 playbooks to configure nodes. Install them along with the plugin manager:\n\n\nyum install vc3-client vc3-infoservice vc3-master pluginmanager ansible vc3-playbooks -y\n\n\n\n\nIf using OpenStack for dynamic head node provisioning, you'll also need python-novaclient from the OpenStack repositories. \n\n\nyum install centos-release-openstack-ocata -y\nyum install python-novaclient -y\n\n\n\n\nAnd configure \n/etc/vc3/vc3-master.conf\n:\n\n\n[DEFAULT]\nloglevel = debug\n\n[master]\ntaskconf=/etc/vc3/tasks.conf\n\n[credible]\ncredconf=/etc/credible/credible.conf\n\n[dynamic]\nplugin=Execute\n\n[netcomm]\nchainfile=/etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\ncertfile=/etc/pki/tls/certs/hostcert.pem\nkeyfile=/etc/pki/tls/private/hostkey.pem\n\ninfohost=info-test.virtualclusters.org\nhttpport=20333\nhttpsport=20334\n\n[core]\nwhitelist = cctools-catalog-server,vc3-factory\n\n\n\n\nYou will also need to modify the client config at \n/etc/vc3/vc3-client.conf\n:\n\n\n[DEFAULT]\nlogLevel = warn\n\n[netcomm]\nchainfile=/etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\ncertfile=/etc/pki/tls/certs/hostcert.pem\nkeyfile=/etc/pki/tls/private/hostkey.pem\n\ninfohost=info-test.virtualclusters.org\nhttpport=20333\nhttpsport=20334\n\n\n\n\nFinally, configure the Master for Openstack/Ansible in \n/etc/vc3/tasks.conf\n:\n\n\n[DEFAULT]\n# in seconds\npolling_interval = 120\n\n[vc3init]\ntaskplugins = InitInstanceAuth,HandlePairingRequests\n\n[vcluster-lifecycle]\n# run at least once per vcluster-requestcycle\ntaskplugins = InitResources,HandleAllocations\npolling_interval = 45\n\n\n[vcluster-requestcycle]\ntaskplugins = HandleRequests\npolling_interval = 60\n\n[consistency-checks]\ntaskplugins = CheckAllocations\n\n[access-checks]\ntaskplugins = CheckResourceAccess\npolling_interval = 360\n\n\n[vcluster-headnodecycle]\n\ntaskplugins = HandleHeadNodes\npolling_interval =  10\n\nusername = myosuser\npassword = secret\nuser_domain_name    = default\nproject_domain_name = default\nauth_url = http://10.32.70.9:5000/v3\n\n#CentOS 7 vanilla minimal install\n#node_image            = 730253d8-d585-43d2-b2d2-16d3af388306\n#CentOS 7 minimal install + condor,cvmfs,gcc,epel,osg-oasis\nnode_image            = 093fd316-fffc-441c-944c-6ba2de582f8f \n\n#large: 2 VCPUS 4GB RAM 10 GB Disk\nnode_flavor           = 344f29c8-7370-49b4-aaf8-b1427582970f \n#small: 1 VCPUS 2GB RAM 10 GB Disk\n#node_flavor           = 15d4a4c3-3b97-409a-91b2-4bc1226382d3\n\nnode_user             = centos\nnode_private_key_file = ~/.ssh/initnode\nnode_public_key_name  = initnode-openstack-name\nnode_security_groups  = ssh,default\nnode_network_id       = 04e64bbe-d017-4aef-928b-0c2c0dd3fc9e\n\nnode_prefix           = dev-\nnode_max_no_contact_time = 900\nnode_max_initializing_count = 3\n\nansible_path         = /etc/vc3/vc3-playbooks/login\nansible_playbook     = login-dynamic.yaml\nansible_debug_file   = /var/log/vc3/ansible.log\n\n\n\n\nNote the \nusername\n and \npassword\n you will need to fill in. You'll also need to put the private key for root on the head nodes here under \n/etc/vc3/keys\n.\n\n\nLaunching the Master\n\n\nOnce the Infoservce has been started, you can start the VC3 Master to process requests. Make sure that the \ninfohost=\n is pointed to the correct hostname in \n/etc/vc3/vc3/master.conf\n.\n\n\nIt may be necessary to create the vc3 log directory (\nsee CORE-140\n) and change permissions on the Credible directory (\nsee CORE-144\n):\n\n\nmkdir -p /var/log/vc3\nchown vc3: /var/log/vc3\n\n\n\n\nFinally, start the service:\n\n\nsystemctl start vc3-master\n\n\n\n\nYou should see something similar to the following if things are working:\n\n\nvc3-master.service - VC3 Master\n   Loaded: loaded (/usr/lib/systemd/system/vc3-master.service; disabled; vendor preset: disabled)\n   Active: active (running) since Fri 2017-09-08 17:38:48 UTC; 48s ago\n\n\n\n\nYou can check to see if things are working with the following command:\n\n\ngrep \nERROR\n /var/log/vc3/*.log || echo \nEverything OK\n\n\n\n\n\n\n\nVC3 Factory\n\n\nThe VC3 factory launches, maintains, and destroys virtual clusters by sending and monitoring pilot jobs that contain the VC3 builder. \n\n\nPrerequisites\n\n\nAs with the master and infoservice, you'll need the VC3 repo installed and public keys distributed before continuing. \n\n\nInstalling the Factory\n\n\nIn addition to the factory itself, you'll need to install VC3-specific plugins, the infoservice and client for APIs, and the pluginmanager. \n\n\nyum install epel-release -y \nyum install autopyfactory vc3-factory-plugins vc3-client vc3-infoservice pluginmanager vc3-remote-manager vc3-builder python-paramiko -y\n\n\n\n\nWe will also need the HTCondor software. Install the public key, repo, and condor package:\n\n\nrpm --import http://research.cs.wisc.edu/htcondor/yum/RPM-GPG-KEY-HTCondor\ncurl http://research.cs.wisc.edu/htcondor/yum/repo.d/htcondor-stable-rhel7.repo \n /etc/yum.repos.d/htcondor-stable-rhel7.repo\nyum install condor\n\n\n\n\nAs before, we will need certificates issued to the Factory host:\n\n\n(master)# credible -c /etc/credible/credible.conf hostcert apf-test.virtualclusters.org\n(factory)# vi /etc/pki/tls/certs/hostcert.pem\n(master)# credible -c /etc/credible/credible.conf hostkey apf-test.virtualclusters.org\n(factory)# vi /etc/pki/tls/private/hostkey.pem\n(master)# credible -c /etc/credible/credible.conf certchain\n(factory)# vi /etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\n\n\n\n\nThe client will need to be configured to point at the test infoservice in \n/etc/vc3/vc3-client.conf\n:\n\n\n[DEFAULT]\nlogLevel = warn\n\n[netcomm]\nchainfile=/etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\ncertfile=/etc/pki/tls/certs/hostcert.pem\nkeyfile=/etc/pki/tls/private/hostkey.pem\n\ninfohost=info-test.virtualclusters.org\nhttpport=20333\nhttpsport=20334\n\n\n\n\nThe factory defaults for VC3, in \n/etc/autopyfactory/vc3defaults.conf\n, may need to be adjusted as follows:\n\n\n[DEFAULT]\nvo = VC3\nstatus = online\noverride = True\nenabled = True\ncleanlogs.keepdays = 7\nbatchstatusplugin = Condor\nwmsstatusplugin = None\nschedplugin = KeepNRunning, MinPerCycle, MaxPerCycle, MaxPending\nsched.maxtorun.maximum = 9999\nsched.maxpending.maximum = 100\nsched.maxpercycle.maximum = 50\nsched.minpercycle.minimum = 0\nsched.keepnrunning.keep_running = 0\nmonitorsection = dummy-monitor\nbuilder = /usr/local/libexec/vc3-builder\n\nperiodic_remove = periodic_remove=(JobStatus == 5 \n (CurrentTime - EnteredCurrentStatus) \n 3600) || (JobStatus == 1 \n globusstatus =!= 1 \n (CurrentTime - EnteredCurrentStatus) \n 86400) || (JobStatus == 2 \n (CurrentTime - EnteredCurrentStatus) \n 604800)\n\nbatchsubmit.condorosgce.proxy = None\nbatchsubmit.condorec2.proxy = None\nbatchsubmit.condorec2.peaceful = True\nbatchsubmit.condorlocal.proxy = None\nbatchsubmit.condorssh.killorder = newest\nbatchsubmit.condorssh.peaceful = False\n\napfqueue.sleep = 60\nbatchstatus.condor.sleep = 25\n\n\n\n\nLikewise, the main \nautopyfactory.conf\n needs to be adjusted:\n\n\n# =================================================================================================================\n#\n# autopyfactory.conf Configuration file for main Factory component of AutoPyFactory.\n#\n# Documentation:\n#   https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactory\n#   https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryConfiguration#5_2_autopyfactory_conf\n#\n# =================================================================================================================\n\n# template for a configuration file\n[Factory]\n\nfactoryAdminEmail = neo@matrix.net\nfactoryId = MYSITE-hostname-sysadminname\nfactorySMTPServer = mail.matrix.net\nfactoryMinEmailRepeatSeconds = 43200\nfactoryUser = autopyfactory\nenablequeues = True\n\nqueueConf = file:///etc/autopyfactory/queues.conf\nqueueDirConf = None\nproxyConf = /etc/autopyfactory/proxy.conf\nauthmanager.enabled = True\nproxymanager.enabled = True\nproxymanager.sleep = 30\nauthmanager.sleep = 30\nauthConf = /etc/autopyfactory/auth.conf\nmonitorConf = /etc/autopyfactory/monitor.conf\nmappingsConf = /etc/autopyfactory/mappings.conf\n\ncycles = 9999999\ncleanlogs.keepdays = 14\n\nfactory.sleep=30\nwmsstatus.panda.sleep = 150\nwmsstatus.panda.maxage = 360\nwmsstatus.condor.sleep = 150\nwmsstatus.condor.maxage = 360\nbatchstatus.condor.sleep = 150\nbatchstatus.condor.maxage = 360\n\nbaseLogDir = /home/autopyfactory/factory/logs\nbaseLogDirUrl = http://myhost.matrix.net:25880\n\nlogserver.enabled = True\nlogserver.index = True\nlogserver.allowrobots = False\n\n# Automatic (re)configuration\nconfig.reconfig = True\nconfig.reconfig.interval = 30\nconfig.queues.plugin = File, VC3\nconfig.auth.plugin = File, VC3\n\nconfig.queues.vc3.vc3clientconf = /etc/vc3/vc3-client.conf\nconfig.queues.vc3.tempfile = ~/queues.conf.tmp\n\n# For static central factory, use 'all' and will check all requests.\nconfig.queues.vc3.requestname = all\nconfig.auth.vc3.vc3clientconf = /etc/vc3/vc3-client.conf\nconfig.auth.vc3.tempfile = ~/auth.conf.tmp\nconfig.auth.vc3.requestname = all\n\n# For the factory-level monitor plugin VC3\nmonitor = VC3\nmonitor.vc3.vc3clientconf = /etc/vc3/vc3-client.conf\n\n\n\n\nStarting the Factory and Condor\n\n\nOnce the Factory, Condor, and the builder have been installed on the factory host, you'll need to start the services:\n\n\nservice condor start\nservice autopyfactory start\n\n\n\n\nAs usual, check for any errors in the factory startup:\n\n\ngrep \nERROR\n /var/log/autopyfactory/*.log || echo \nEverything OK\n\n\n\n\n\nMonitoring the pilots\n\n\nWe use a graphite server provided by MWT2 to plot time series data. Create the monitoring script in \n/usr/local/bin/monitor-pilots.sh\n:\n\n\n#!/bin/bash\ncondor_q -nobatch -global -const 'Jobstatus == 1' -long | grep \nMATCH_APF\n | sort | uniq -c | sed 's/\\\n//g' |awk -v date=$(date +%s) -v hostname=$(hostname | tr '.' '_') '{ print \ncondor.factory.\nhostname\n.idle.\n $4,$1,date}' | nc -w 30 graphite.mwt2.org 2003\ncondor_q -nobatch -global -const 'Jobstatus == 2' -long | grep \nMATCH_APF\n | sort | uniq -c | sed 's/\\\n//g' |awk -v date=$(date +%s) -v hostname=$(hostname | tr '.' '_') '{ print \ncondor.factory.\nhostname\n.running.\n $4,$1,date}' | nc -w 30 graphite.mwt2.org 2003\n\n\n\n\nMake it executable, install \nnc\n if necessary, and test for any errors:\n\n\nchmod +x /usr/local/bin/monitor-pilots.sh\nyum install nc -y\n/usr/local/bin/monitor-pilots.sh\n\n\n\n\nIf successful, there should be no output. Finally add it to root's crontab (\ncrontab -e\n as root) with the following cron entry:\n\n\n* * * * * /usr/local/bin/monitor-pilots.sh", 
            "title": "VC3 Infrastructure"
        }, 
        {
            "location": "/devguide/deployment-guide/#prerequisites-for-all-services", 
            "text": "", 
            "title": "Prerequisites for all services"
        }, 
        {
            "location": "/devguide/deployment-guide/#adding-keys", 
            "text": "When a piece of static infrastructure is initialized on OpenStack or AWS, it will only contain  your  public key at first. You'll need to add the public keys for other developers as well.  vi .ssh/authorized_keys  The CentOS account should, by default, have  sudo  privileges.", 
            "title": "Adding keys"
        }, 
        {
            "location": "/devguide/deployment-guide/#installing-the-repos", 
            "text": "On all pieces of the static infrastructure, you will need to install the VC3 package repository. Add the following contents to  /etc/yum.repos.d/vc3.repo :  [vc3-x86_64]\nname=VC3 x86_64\nbaseurl=http://build.virtualclusters.org/production/x86_64\ngpgcheck=0\n[vc3-noarch]\nname=VC3 noarch\nbaseurl=http://build.virtualclusters.org/production/noarch\ngpgcheck=0", 
            "title": "Installing the repos"
        }, 
        {
            "location": "/devguide/deployment-guide/#bootstrapping-authentication-on-the-master", 
            "text": "", 
            "title": "Bootstrapping authentication on the Master"
        }, 
        {
            "location": "/devguide/deployment-guide/#installing-credible-and-setting-up-certificates", 
            "text": "Credible will be used to issue certificates to all VC3 components for authentication. We will only set it up on the Master, and then copy certificates and keys to other pieces of the infrastructure.   Install credible from the repos:  yum install credible  And update  /etc/credible/credible.conf  as appropriate:  [credible]\nstorageplugin = Memory\nvardir = /var/credible\n\n[credible-ssh]\nkeytype = rsa\nbitlength = 4096\n\n[credible-ssca]\nroottemplate=/etc/credible/openssl.cnf.root.template\nintermediatetemplate=/etc/credible/openssl.cnf.intermediate.template\nvardir = /var/credible\nbitlength = 4096\n\n# Test defaults.\nsscaname = VC3\ncountry = US\nlocality = Upton\nstate = NY\norganization = BNL\norgunit = SDCC\nemail = jhover@bnl.gov", 
            "title": "Installing Credible and setting up certificates"
        }, 
        {
            "location": "/devguide/deployment-guide/#issuing-certificates", 
            "text": "Retrieve the CA Chain and generate a host certificate and key for the Master:  credible -c /etc/credible/credible.conf hostcert master-test.virtualclusters.org   /etc/pki/tls/certs/hostcert.pem\ncredible -c /etc/credible/credible.conf hostkey master-test.virtualclusters.org   /etc/pki/tls/private/hostkey.pem\ncredible -c /etc/credible/credible.conf certchain   /etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem", 
            "title": "Issuing certificates"
        }, 
        {
            "location": "/devguide/deployment-guide/#vc3-infoservice", 
            "text": "", 
            "title": "VC3 Infoservice"
        }, 
        {
            "location": "/devguide/deployment-guide/#prerequisites", 
            "text": "As with the Master, you will need to install the VC3 repo and any public keys.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/devguide/deployment-guide/#installing-the-infoservice", 
            "text": "Assuming you have already configured the VC3 repos, install the following:  yum install epel-release -y\nyum install vc3-infoservice pluginmanager python-pip openssl -y\npip install pyOpenSSL CherryPy==11.0.0  And edit config at  /etc/vc3/vc3-infoservice.conf :  [DEFAULT]\nloglevel = debug\n\n[netcomm]\nchainfile=/etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\ncertfile=/etc/pki/tls/certs/hostcert.pem\nkeyfile=/etc/pki/tls/private/hostkey.pem\n\nsslmodule=pyopenssl\nhttpport=20333\nhttpsport=20334\n\n[persistence]\nplugin = DiskDump\n\n[plugin-diskdump]\nfilename=/tmp/infoservice.diskdump  On the  Master  host, you will need to issue certificates for the Infoservice. Copy and paste into the appropriate files:  (master)# credible -c /etc/credible/credible.conf hostcert info-test.virtualclusters.org\n(infoservice)# vi /etc/pki/tls/certs/hostcert.pem\n(master)# credible -c /etc/credible/credible.conf hostkey info-test.virtualclusters.org\n(infoservice)# vi /etc/pki/tls/private/hostkey.pem\n(master)# credible -c /etc/credible/credible.conf certchain\n(infoservice)# /etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem", 
            "title": "Installing the Infoservice"
        }, 
        {
            "location": "/devguide/deployment-guide/#starting-the-infoservice", 
            "text": "Due to a bug (CORE-261), we need to create  /var/log/vc3  mkdir -p /var/log/vc3  For now, we'll use the sysv-init style startup scripts:  /etc/init.d/vc3-infoservice.init start  If it's running, you should see the following after  /etc/init.d/vc3-infoservice.init status :  \u25cf vc3-infoservice.init.service - LSB: start and stop vc3-info-service\n   Loaded: loaded (/etc/rc.d/init.d/vc3-infoservice.init; bad; vendor preset: disabled)\n   Active: active (running) since Fri 2017-09-08 16:31:13 UTC; 36s ago  Check the logs for any ERROR statements:  grep  ERROR  /var/log/vc3/*log || echo  Everything OK", 
            "title": "Starting the Infoservice"
        }, 
        {
            "location": "/devguide/deployment-guide/#vc3-master", 
            "text": "", 
            "title": "VC3 Master"
        }, 
        {
            "location": "/devguide/deployment-guide/#installing-the-master", 
            "text": "The Master depends on the vc3-client and vc3-infoservice packages for the client APIs. We also need ansible and the VC3 playbooks to configure nodes. Install them along with the plugin manager:  yum install vc3-client vc3-infoservice vc3-master pluginmanager ansible vc3-playbooks -y  If using OpenStack for dynamic head node provisioning, you'll also need python-novaclient from the OpenStack repositories.   yum install centos-release-openstack-ocata -y\nyum install python-novaclient -y  And configure  /etc/vc3/vc3-master.conf :  [DEFAULT]\nloglevel = debug\n\n[master]\ntaskconf=/etc/vc3/tasks.conf\n\n[credible]\ncredconf=/etc/credible/credible.conf\n\n[dynamic]\nplugin=Execute\n\n[netcomm]\nchainfile=/etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\ncertfile=/etc/pki/tls/certs/hostcert.pem\nkeyfile=/etc/pki/tls/private/hostkey.pem\n\ninfohost=info-test.virtualclusters.org\nhttpport=20333\nhttpsport=20334\n\n[core]\nwhitelist = cctools-catalog-server,vc3-factory  You will also need to modify the client config at  /etc/vc3/vc3-client.conf :  [DEFAULT]\nlogLevel = warn\n\n[netcomm]\nchainfile=/etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\ncertfile=/etc/pki/tls/certs/hostcert.pem\nkeyfile=/etc/pki/tls/private/hostkey.pem\n\ninfohost=info-test.virtualclusters.org\nhttpport=20333\nhttpsport=20334  Finally, configure the Master for Openstack/Ansible in  /etc/vc3/tasks.conf :  [DEFAULT]\n# in seconds\npolling_interval = 120\n\n[vc3init]\ntaskplugins = InitInstanceAuth,HandlePairingRequests\n\n[vcluster-lifecycle]\n# run at least once per vcluster-requestcycle\ntaskplugins = InitResources,HandleAllocations\npolling_interval = 45\n\n\n[vcluster-requestcycle]\ntaskplugins = HandleRequests\npolling_interval = 60\n\n[consistency-checks]\ntaskplugins = CheckAllocations\n\n[access-checks]\ntaskplugins = CheckResourceAccess\npolling_interval = 360\n\n\n[vcluster-headnodecycle]\n\ntaskplugins = HandleHeadNodes\npolling_interval =  10\n\nusername = myosuser\npassword = secret\nuser_domain_name    = default\nproject_domain_name = default\nauth_url = http://10.32.70.9:5000/v3\n\n#CentOS 7 vanilla minimal install\n#node_image            = 730253d8-d585-43d2-b2d2-16d3af388306\n#CentOS 7 minimal install + condor,cvmfs,gcc,epel,osg-oasis\nnode_image            = 093fd316-fffc-441c-944c-6ba2de582f8f \n\n#large: 2 VCPUS 4GB RAM 10 GB Disk\nnode_flavor           = 344f29c8-7370-49b4-aaf8-b1427582970f \n#small: 1 VCPUS 2GB RAM 10 GB Disk\n#node_flavor           = 15d4a4c3-3b97-409a-91b2-4bc1226382d3\n\nnode_user             = centos\nnode_private_key_file = ~/.ssh/initnode\nnode_public_key_name  = initnode-openstack-name\nnode_security_groups  = ssh,default\nnode_network_id       = 04e64bbe-d017-4aef-928b-0c2c0dd3fc9e\n\nnode_prefix           = dev-\nnode_max_no_contact_time = 900\nnode_max_initializing_count = 3\n\nansible_path         = /etc/vc3/vc3-playbooks/login\nansible_playbook     = login-dynamic.yaml\nansible_debug_file   = /var/log/vc3/ansible.log  Note the  username  and  password  you will need to fill in. You'll also need to put the private key for root on the head nodes here under  /etc/vc3/keys .", 
            "title": "Installing the Master"
        }, 
        {
            "location": "/devguide/deployment-guide/#launching-the-master", 
            "text": "Once the Infoservce has been started, you can start the VC3 Master to process requests. Make sure that the  infohost=  is pointed to the correct hostname in  /etc/vc3/vc3/master.conf .  It may be necessary to create the vc3 log directory ( see CORE-140 ) and change permissions on the Credible directory ( see CORE-144 ):  mkdir -p /var/log/vc3\nchown vc3: /var/log/vc3  Finally, start the service:  systemctl start vc3-master  You should see something similar to the following if things are working:  vc3-master.service - VC3 Master\n   Loaded: loaded (/usr/lib/systemd/system/vc3-master.service; disabled; vendor preset: disabled)\n   Active: active (running) since Fri 2017-09-08 17:38:48 UTC; 48s ago  You can check to see if things are working with the following command:  grep  ERROR  /var/log/vc3/*.log || echo  Everything OK", 
            "title": "Launching the Master"
        }, 
        {
            "location": "/devguide/deployment-guide/#vc3-factory", 
            "text": "The VC3 factory launches, maintains, and destroys virtual clusters by sending and monitoring pilot jobs that contain the VC3 builder.", 
            "title": "VC3 Factory"
        }, 
        {
            "location": "/devguide/deployment-guide/#prerequisites_1", 
            "text": "As with the master and infoservice, you'll need the VC3 repo installed and public keys distributed before continuing.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/devguide/deployment-guide/#installing-the-factory", 
            "text": "In addition to the factory itself, you'll need to install VC3-specific plugins, the infoservice and client for APIs, and the pluginmanager.   yum install epel-release -y \nyum install autopyfactory vc3-factory-plugins vc3-client vc3-infoservice pluginmanager vc3-remote-manager vc3-builder python-paramiko -y  We will also need the HTCondor software. Install the public key, repo, and condor package:  rpm --import http://research.cs.wisc.edu/htcondor/yum/RPM-GPG-KEY-HTCondor\ncurl http://research.cs.wisc.edu/htcondor/yum/repo.d/htcondor-stable-rhel7.repo   /etc/yum.repos.d/htcondor-stable-rhel7.repo\nyum install condor  As before, we will need certificates issued to the Factory host:  (master)# credible -c /etc/credible/credible.conf hostcert apf-test.virtualclusters.org\n(factory)# vi /etc/pki/tls/certs/hostcert.pem\n(master)# credible -c /etc/credible/credible.conf hostkey apf-test.virtualclusters.org\n(factory)# vi /etc/pki/tls/private/hostkey.pem\n(master)# credible -c /etc/credible/credible.conf certchain\n(factory)# vi /etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem  The client will need to be configured to point at the test infoservice in  /etc/vc3/vc3-client.conf :  [DEFAULT]\nlogLevel = warn\n\n[netcomm]\nchainfile=/etc/pki/ca-trust/extracted/pem/vc3-chain.cert.pem\ncertfile=/etc/pki/tls/certs/hostcert.pem\nkeyfile=/etc/pki/tls/private/hostkey.pem\n\ninfohost=info-test.virtualclusters.org\nhttpport=20333\nhttpsport=20334  The factory defaults for VC3, in  /etc/autopyfactory/vc3defaults.conf , may need to be adjusted as follows:  [DEFAULT]\nvo = VC3\nstatus = online\noverride = True\nenabled = True\ncleanlogs.keepdays = 7\nbatchstatusplugin = Condor\nwmsstatusplugin = None\nschedplugin = KeepNRunning, MinPerCycle, MaxPerCycle, MaxPending\nsched.maxtorun.maximum = 9999\nsched.maxpending.maximum = 100\nsched.maxpercycle.maximum = 50\nsched.minpercycle.minimum = 0\nsched.keepnrunning.keep_running = 0\nmonitorsection = dummy-monitor\nbuilder = /usr/local/libexec/vc3-builder\n\nperiodic_remove = periodic_remove=(JobStatus == 5   (CurrentTime - EnteredCurrentStatus)   3600) || (JobStatus == 1   globusstatus =!= 1   (CurrentTime - EnteredCurrentStatus)   86400) || (JobStatus == 2   (CurrentTime - EnteredCurrentStatus)   604800)\n\nbatchsubmit.condorosgce.proxy = None\nbatchsubmit.condorec2.proxy = None\nbatchsubmit.condorec2.peaceful = True\nbatchsubmit.condorlocal.proxy = None\nbatchsubmit.condorssh.killorder = newest\nbatchsubmit.condorssh.peaceful = False\n\napfqueue.sleep = 60\nbatchstatus.condor.sleep = 25  Likewise, the main  autopyfactory.conf  needs to be adjusted:  # =================================================================================================================\n#\n# autopyfactory.conf Configuration file for main Factory component of AutoPyFactory.\n#\n# Documentation:\n#   https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactory\n#   https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryConfiguration#5_2_autopyfactory_conf\n#\n# =================================================================================================================\n\n# template for a configuration file\n[Factory]\n\nfactoryAdminEmail = neo@matrix.net\nfactoryId = MYSITE-hostname-sysadminname\nfactorySMTPServer = mail.matrix.net\nfactoryMinEmailRepeatSeconds = 43200\nfactoryUser = autopyfactory\nenablequeues = True\n\nqueueConf = file:///etc/autopyfactory/queues.conf\nqueueDirConf = None\nproxyConf = /etc/autopyfactory/proxy.conf\nauthmanager.enabled = True\nproxymanager.enabled = True\nproxymanager.sleep = 30\nauthmanager.sleep = 30\nauthConf = /etc/autopyfactory/auth.conf\nmonitorConf = /etc/autopyfactory/monitor.conf\nmappingsConf = /etc/autopyfactory/mappings.conf\n\ncycles = 9999999\ncleanlogs.keepdays = 14\n\nfactory.sleep=30\nwmsstatus.panda.sleep = 150\nwmsstatus.panda.maxage = 360\nwmsstatus.condor.sleep = 150\nwmsstatus.condor.maxage = 360\nbatchstatus.condor.sleep = 150\nbatchstatus.condor.maxage = 360\n\nbaseLogDir = /home/autopyfactory/factory/logs\nbaseLogDirUrl = http://myhost.matrix.net:25880\n\nlogserver.enabled = True\nlogserver.index = True\nlogserver.allowrobots = False\n\n# Automatic (re)configuration\nconfig.reconfig = True\nconfig.reconfig.interval = 30\nconfig.queues.plugin = File, VC3\nconfig.auth.plugin = File, VC3\n\nconfig.queues.vc3.vc3clientconf = /etc/vc3/vc3-client.conf\nconfig.queues.vc3.tempfile = ~/queues.conf.tmp\n\n# For static central factory, use 'all' and will check all requests.\nconfig.queues.vc3.requestname = all\nconfig.auth.vc3.vc3clientconf = /etc/vc3/vc3-client.conf\nconfig.auth.vc3.tempfile = ~/auth.conf.tmp\nconfig.auth.vc3.requestname = all\n\n# For the factory-level monitor plugin VC3\nmonitor = VC3\nmonitor.vc3.vc3clientconf = /etc/vc3/vc3-client.conf", 
            "title": "Installing the Factory"
        }, 
        {
            "location": "/devguide/deployment-guide/#starting-the-factory-and-condor", 
            "text": "Once the Factory, Condor, and the builder have been installed on the factory host, you'll need to start the services:  service condor start\nservice autopyfactory start  As usual, check for any errors in the factory startup:  grep  ERROR  /var/log/autopyfactory/*.log || echo  Everything OK", 
            "title": "Starting the Factory and Condor"
        }, 
        {
            "location": "/devguide/deployment-guide/#monitoring-the-pilots", 
            "text": "We use a graphite server provided by MWT2 to plot time series data. Create the monitoring script in  /usr/local/bin/monitor-pilots.sh :  #!/bin/bash\ncondor_q -nobatch -global -const 'Jobstatus == 1' -long | grep  MATCH_APF  | sort | uniq -c | sed 's/\\ //g' |awk -v date=$(date +%s) -v hostname=$(hostname | tr '.' '_') '{ print  condor.factory. hostname .idle.  $4,$1,date}' | nc -w 30 graphite.mwt2.org 2003\ncondor_q -nobatch -global -const 'Jobstatus == 2' -long | grep  MATCH_APF  | sort | uniq -c | sed 's/\\ //g' |awk -v date=$(date +%s) -v hostname=$(hostname | tr '.' '_') '{ print  condor.factory. hostname .running.  $4,$1,date}' | nc -w 30 graphite.mwt2.org 2003  Make it executable, install  nc  if necessary, and test for any errors:  chmod +x /usr/local/bin/monitor-pilots.sh\nyum install nc -y\n/usr/local/bin/monitor-pilots.sh  If successful, there should be no output. Finally add it to root's crontab ( crontab -e  as root) with the following cron entry:  * * * * * /usr/local/bin/monitor-pilots.sh", 
            "title": "Monitoring the pilots"
        }, 
        {
            "location": "/devguide/deployment-web/", 
            "text": "VC3 Web Portal\n\n\nThe VC3 web portal is a flask application that integrates the VC3 client APIs to give end-users a GUI for instantiating, running and terminating virtual clusters, registering resources and allocations, managing projects, and more.\n\n\nPrerequisites\n\n\nFor the host, you will need to install the development public keys and issue certs by the VC3 master. \n\n\nAll of the web portal's non-secret dependencies are included in a Docker container. However, you will need the Docker engine running, as well as a certificate issued by LetsEncrypt or another CA if you want the website to actually be visible over HTTPS without warnings.\n\n\nInstalling and running Docker\n\n\nYou will first need to install EPEL and the Docker engine\n\n\nyum install epel-release -y\nyum install docker -y\n\n\n\n\nAnd start the service:\n\n\nsystemctl start docker \nsystemctl status docker\n\n\n\n\nIf it's working, you should see something like this:\n\n\n\u25cf docker.service - Docker Application Container Engine\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)\n   Active: active (running) since Mon 2017-09-11 19:23:18 UTC; 2s ago\n     Docs: http://docs.docker.com\n Main PID: 16879 (dockerd-current)\n\n\n\n\nIssuing a certificate with Let's Encrypt\n\n\nWe use Certbot to issue SSL certificates that have been trusted by Let's Encrypt. First, install Cerbot:\n\n\nyum install certbot -y\n\n\n\n\nNext, run \ncertbot certonly\n and go through the prompts:\n\n\n[root@www-test ~]# certbot certonly\nSaving debug log to /var/log/letsencrypt/letsencrypt.log\n\nHow would you like to authenticate with the ACME CA?\n-------------------------------------------------------------------------------\n1: Spin up a temporary webserver (standalone)\n2: Place files in webroot directory (webroot)\n-------------------------------------------------------------------------------\nSelect the appropriate number [1-2] then [enter] (press 'c' to cancel): 1\nEnter email address (used for urgent renewal and security notices) (Enter 'c' to\ncancel):lincolnb@uchicago.edu\nStarting new HTTPS connection (1): acme-v01.api.letsencrypt.org\n\n-------------------------------------------------------------------------------\nPlease read the Terms of Service at\nhttps://letsencrypt.org/documents/LE-SA-v1.1.1-August-1-2016.pdf. You must agree\nin order to register with the ACME server at\nhttps://acme-v01.api.letsencrypt.org/directory\n-------------------------------------------------------------------------------\n(A)gree/(C)ancel: A\n\n-------------------------------------------------------------------------------\nWould you be willing to share your email address with the Electronic Frontier\nFoundation, a founding partner of the Let's Encrypt project and the non-profit\norganization that develops Certbot? We'd like to send you email about EFF and\nour work to encrypt the web, protect its users and defend digital rights.\n-------------------------------------------------------------------------------\n(Y)es/(N)o: n\nPlease enter in your domain name(s) (comma and/or space separated)  (Enter 'c'\nto cancel):www-test.virtualclusters.org\nObtaining a new certificate\nPerforming the following challenges:\ntls-sni-01 challenge for www-test.virtualclusters.org\nWaiting for verification...\nCleaning up challenges\n\nIMPORTANT NOTES:\n - Congratulations! Your certificate and chain have been saved at\n   /etc/letsencrypt/live/www-test.virtualclusters.org/fullchain.pem.\n   Your cert will expire on 2017-12-10. To obtain a new or tweaked\n   version of this certificate in the future, simply run certbot\n   again. To non-interactively renew *all* of your certificates, run\n   \ncertbot renew\n\n - Your account credentials have been saved in your Certbot\n   configuration directory at /etc/letsencrypt. You should make a\n   secure backup of this folder now. This configuration directory will\n   also contain certificates and private keys obtained by Certbot so\n   making regular backups of this folder is ideal.\n - If you like Certbot, please consider supporting our work by:\n\n   Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate\n   Donating to EFF:                    https://eff.org/donate-le\n\n\n\n\nTake note of the fullchain location, e.g. \n/etc/letsencrypt/live/www-test.virtualclusters.org/fullchain.pem\n. You will need this location for the Container.\n\n\nInstalling the web portal\n\n\nFirst, install git\n\n\nyum install git -y\n\n\n\n\nThen clone the web portal: \n\n\ncd ~\ngit clone https://github.com/vc3-project/vc3-website-python\n\n\n\n\nInstalling the secrets\n\n\nYou'll need 3 sets of secrets, at the following locations:\n * \n/root/secrets/$(hostname)/\n\n * \n/root/secrets/portal.conf\n \n * \n/root/secrets/vc3/\n\n\nDocker can have unusual behavior with symbolic links, so it's best to just copy the WWW certs into place:\n\n\nmkdir -p /root/secrets/$(hostname)\ncp -rL /etc/letsencrypt/live/$(hostname)/* /root/secrets/$(hostname)\n\n\n\n\nThe \nportal.conf\n should be obtained from the VC3 developers, or re-issued from Globus if necessary.\n\n\nFor the VC3 certificates, you'll need to issue them with the Master as usual. The container expects the following filenames:\n * \nlocalhost.cert.pem\n\n * \nlocalhost.keynopw.pem\n\n * \nvc3chain.pem\n\n\nAs before:\n\n\n(master)# credible -c /etc/credible/credible.conf hostcert apf-test.virtualclusters.org\n(www)# vi /root/secrets/vc3/localhost.cert.pem\n(master)# credible -c /etc/credible/credible.conf hostkey apf-test.virtualclusters.org\n(www)# vi /root/secrets/vc3/localhost.keynopw.pem\n(master)# credible -c /etc/credible/credible.conf certchain\n(www)# vi /root/secrets/vc3/vc3chain.pem\n\n\n\n\nRunning the container\n\n\nOnce you have issued your certificates, you'll need to deploy the container and mount the secrets into it at run-time. We intentionally separate secrets such that everything else can be dumped into Github.\n\n\nUse the following systemd file\n\n\n[Unit]\nDescription=VC3 Development Website\nAfter=syslog.target network.target\n\n[Service]\nType=simple\nExecStartPre=/usr/local/bin/update_dev_code.sh\nExecStart=/usr/bin/docker run --rm --name vc3-portal -p 80:8080 -p 443:4443 -v /root/vc3-website/secrets/www-dev.virtualclusters.org:/etc/letsencrypt/live/virtualclusters.org -v /root/vc3-website/secrets/portal.conf:/srv/www/vc3-web-env/portal/portal.conf -v /root/vc3-website/secrets/vc3:/srv/www/vc3-web-env/etc/certs -v /root/vc3-website-python:/srv/www/vc3-web-env virtualclusters/vc3-portal:latest\nExecReload=/usr/bin/docker restart vc3-portal\nExecStop=/usr/bin/docker stop vc3-portal\n\n\n\n\nOnce placed in \n/etc/systemd/system/website.service\n, \nsystemctl start\n, \nsystemctl stop\n, and \nsystemctl restart\n will do the appropriate things.", 
            "title": "VC3 Website"
        }, 
        {
            "location": "/devguide/deployment-web/#vc3-web-portal", 
            "text": "The VC3 web portal is a flask application that integrates the VC3 client APIs to give end-users a GUI for instantiating, running and terminating virtual clusters, registering resources and allocations, managing projects, and more.", 
            "title": "VC3 Web Portal"
        }, 
        {
            "location": "/devguide/deployment-web/#prerequisites", 
            "text": "For the host, you will need to install the development public keys and issue certs by the VC3 master.   All of the web portal's non-secret dependencies are included in a Docker container. However, you will need the Docker engine running, as well as a certificate issued by LetsEncrypt or another CA if you want the website to actually be visible over HTTPS without warnings.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/devguide/deployment-web/#installing-and-running-docker", 
            "text": "You will first need to install EPEL and the Docker engine  yum install epel-release -y\nyum install docker -y  And start the service:  systemctl start docker \nsystemctl status docker  If it's working, you should see something like this:  \u25cf docker.service - Docker Application Container Engine\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)\n   Active: active (running) since Mon 2017-09-11 19:23:18 UTC; 2s ago\n     Docs: http://docs.docker.com\n Main PID: 16879 (dockerd-current)", 
            "title": "Installing and running Docker"
        }, 
        {
            "location": "/devguide/deployment-web/#issuing-a-certificate-with-lets-encrypt", 
            "text": "We use Certbot to issue SSL certificates that have been trusted by Let's Encrypt. First, install Cerbot:  yum install certbot -y  Next, run  certbot certonly  and go through the prompts:  [root@www-test ~]# certbot certonly\nSaving debug log to /var/log/letsencrypt/letsencrypt.log\n\nHow would you like to authenticate with the ACME CA?\n-------------------------------------------------------------------------------\n1: Spin up a temporary webserver (standalone)\n2: Place files in webroot directory (webroot)\n-------------------------------------------------------------------------------\nSelect the appropriate number [1-2] then [enter] (press 'c' to cancel): 1\nEnter email address (used for urgent renewal and security notices) (Enter 'c' to\ncancel):lincolnb@uchicago.edu\nStarting new HTTPS connection (1): acme-v01.api.letsencrypt.org\n\n-------------------------------------------------------------------------------\nPlease read the Terms of Service at\nhttps://letsencrypt.org/documents/LE-SA-v1.1.1-August-1-2016.pdf. You must agree\nin order to register with the ACME server at\nhttps://acme-v01.api.letsencrypt.org/directory\n-------------------------------------------------------------------------------\n(A)gree/(C)ancel: A\n\n-------------------------------------------------------------------------------\nWould you be willing to share your email address with the Electronic Frontier\nFoundation, a founding partner of the Let's Encrypt project and the non-profit\norganization that develops Certbot? We'd like to send you email about EFF and\nour work to encrypt the web, protect its users and defend digital rights.\n-------------------------------------------------------------------------------\n(Y)es/(N)o: n\nPlease enter in your domain name(s) (comma and/or space separated)  (Enter 'c'\nto cancel):www-test.virtualclusters.org\nObtaining a new certificate\nPerforming the following challenges:\ntls-sni-01 challenge for www-test.virtualclusters.org\nWaiting for verification...\nCleaning up challenges\n\nIMPORTANT NOTES:\n - Congratulations! Your certificate and chain have been saved at\n   /etc/letsencrypt/live/www-test.virtualclusters.org/fullchain.pem.\n   Your cert will expire on 2017-12-10. To obtain a new or tweaked\n   version of this certificate in the future, simply run certbot\n   again. To non-interactively renew *all* of your certificates, run\n    certbot renew \n - Your account credentials have been saved in your Certbot\n   configuration directory at /etc/letsencrypt. You should make a\n   secure backup of this folder now. This configuration directory will\n   also contain certificates and private keys obtained by Certbot so\n   making regular backups of this folder is ideal.\n - If you like Certbot, please consider supporting our work by:\n\n   Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate\n   Donating to EFF:                    https://eff.org/donate-le  Take note of the fullchain location, e.g.  /etc/letsencrypt/live/www-test.virtualclusters.org/fullchain.pem . You will need this location for the Container.", 
            "title": "Issuing a certificate with Let's Encrypt"
        }, 
        {
            "location": "/devguide/deployment-web/#installing-the-web-portal", 
            "text": "First, install git  yum install git -y  Then clone the web portal:   cd ~\ngit clone https://github.com/vc3-project/vc3-website-python", 
            "title": "Installing the web portal"
        }, 
        {
            "location": "/devguide/deployment-web/#installing-the-secrets", 
            "text": "You'll need 3 sets of secrets, at the following locations:\n *  /root/secrets/$(hostname)/ \n *  /root/secrets/portal.conf  \n *  /root/secrets/vc3/  Docker can have unusual behavior with symbolic links, so it's best to just copy the WWW certs into place:  mkdir -p /root/secrets/$(hostname)\ncp -rL /etc/letsencrypt/live/$(hostname)/* /root/secrets/$(hostname)  The  portal.conf  should be obtained from the VC3 developers, or re-issued from Globus if necessary.  For the VC3 certificates, you'll need to issue them with the Master as usual. The container expects the following filenames:\n *  localhost.cert.pem \n *  localhost.keynopw.pem \n *  vc3chain.pem  As before:  (master)# credible -c /etc/credible/credible.conf hostcert apf-test.virtualclusters.org\n(www)# vi /root/secrets/vc3/localhost.cert.pem\n(master)# credible -c /etc/credible/credible.conf hostkey apf-test.virtualclusters.org\n(www)# vi /root/secrets/vc3/localhost.keynopw.pem\n(master)# credible -c /etc/credible/credible.conf certchain\n(www)# vi /root/secrets/vc3/vc3chain.pem", 
            "title": "Installing the secrets"
        }, 
        {
            "location": "/devguide/deployment-web/#running-the-container", 
            "text": "Once you have issued your certificates, you'll need to deploy the container and mount the secrets into it at run-time. We intentionally separate secrets such that everything else can be dumped into Github.  Use the following systemd file  [Unit]\nDescription=VC3 Development Website\nAfter=syslog.target network.target\n\n[Service]\nType=simple\nExecStartPre=/usr/local/bin/update_dev_code.sh\nExecStart=/usr/bin/docker run --rm --name vc3-portal -p 80:8080 -p 443:4443 -v /root/vc3-website/secrets/www-dev.virtualclusters.org:/etc/letsencrypt/live/virtualclusters.org -v /root/vc3-website/secrets/portal.conf:/srv/www/vc3-web-env/portal/portal.conf -v /root/vc3-website/secrets/vc3:/srv/www/vc3-web-env/etc/certs -v /root/vc3-website-python:/srv/www/vc3-web-env virtualclusters/vc3-portal:latest\nExecReload=/usr/bin/docker restart vc3-portal\nExecStop=/usr/bin/docker stop vc3-portal  Once placed in  /etc/systemd/system/website.service ,  systemctl start ,  systemctl stop , and  systemctl restart  will do the appropriate things.", 
            "title": "Running the container"
        }, 
        {
            "location": "/glossary/", 
            "text": "Glossary and Concepts used in VC3 Documentation\n\n\nPortal Users\n\n\nAllocation\n\n\n\n\nAn Allocation refers to both User and Resource concepts. Each Allocation must be owned by an User. Allocations are divisible/fractionable, and can be given to Projects. Allocations may not be oversubscribed. However, unbounded Allocations may be parents of multiple unbounded SubAllocations. Bounded Allocations cannot spawn unbounded SubAllocations. If a Resource grants hard allocation and allows backfill mode, those are two distinct Allocations (one hard and one unbounded)\n\n\n\n\nAuthentication\n\n\n\n\nThe current mechanism for users to sign-up and create accounts on the VC3 platform is by authenticating themselves with their \nGlobusID\n.\n\n\n\n\nMFA\n\n\n\n\nMulti-factor authentication, c.f. \nWikipedia\n.\n\n\n\n\nProject\n\n\n\n\nA collection of \u201cAllocations\u201d from individual users. It has at least 1 \u201cuser owner\u201d, and 0 or more non-owner members. The owner is also a member.\n\n\n\n\nRequest\n\n\n\n\nEntity that encapsulates all information that defines a particular virtual cluster. Creating a new Request triggers creation of the cluster.\n\n\n\n\nRequest templates\n\n\n\n\na list of pre-existing forms to be used as base for new cluster requests creation.\n\n\n\n\nResource\n\n\n\n\nAny target on which a vc3-builder will run to provide computing power to a virtual cluster.\n\n\n\n\nResource profiles\n\n\n\n\na list of pre-existing forms to be used as base for new resource definition.\n\n\n\n\nService unit\n\n\n\n\n\n\nService units are essentially just walltime hours, with minimum charges based on minimum cores or minimum nodes per job. Much like HEPSPEC, the SUs can be normalized/converted based on LINPACK benchmarks. Doc from XSEDE: https://portal.xsede.org/knowledge-base/-/kb/document/bazo\n\n\n\n\n\n\nFor storage, possibly with multiple allocations per user, examples are scratch disk vs long term storage.\n\n\n\n\n\n\nExotic devices like GPUs may or may not be accounted for, depending on the resource.\n\n\n\n\n\n\nSub-Allocation\n\n\n\n\nA SubAllocation can be defined in terms of fraction or units (cpuhours?, $dollars, HEPSPEC) or be unbounded. SubAllocations are children of an Allocation.\n\n\n\n\nUser\n\n\n\n\nEvery User has 0 or more Allocations. Users are owners or members of one or more projects. A User in a project can make Request(s) utilizing project member\u2019s Allocations\n\n\n\n\nProject Developers\n\n\ncluster states\n\n\nList of each possible state of a cluster throughout its lifecycle:\n\n\n\n\nNEW - Request was just created.\n\n\nVALIDATED - Request has been validated for basic correctness.\n\n\nPENDING - Request is valid and is waiting to be instantiated.\n\n\nGROWING - Cluster is in the process of being instantiated but is not yet usable.\n\n\nRUNNING - Cluster is ready to use.\n\n\nSHRINKING - Cluster resources are being removed.\n\n\nTERMINATING - Cluster is about to be destroyed.\n\n\nTERMINATED - Cluster no longer exists.\n\n\n\n\ncredible\n\n\n\n\nCredible is a 3rd-party utility for programmatically generating, storing, and retrieving security tokens.\n\n\n\n\ndynamic infrastructure\n\n\n\n\nServices that are instantiated upon a virtual cluster request, such as the factory.\n\n\n\n\nfactory\n\n\n\n\nThe scheduler and resource manager for middleware.\n\n\n\n\nformatter\n\n\n\n\nA plugin that augments the output of |Flake8| when passed to flake8 --format.\n\n\n\n\ninfo service\n\n\n\n\n\n\nLong-running daemon that interacts with the information database on behalf of other services.\n\n\n\n\n\n\nThe VC3 info service serves as both a persistence mechanism for the overall service, and a message bus between components. Information is stored and retrieved in the form of JSON-formatted documents, which thus form a single tree of information entities/nodes. The service optionally allows access security by enforcing ACLs at each node level.\n\n\n\n\n\n\nPIN\n\n\n\n\nPersonal Identification Number. One-time password for configuring a VC3 resource via vc3-resource-tool\n\n\n\n\nplugin-manager\n\n\n\n\nThe plugin manager is a 3rd-party small utility for quickly constructing plugin objects from configuration input.\n\n\n\n\nrequest ID\n\n\n\n\nUnique identifier for a virtual cluster request.\n\n\n\n\nstatic infrastructure\n\n\n\n\nA set of long-running services, such as the Info Service, Master, etc.\n\n\n\n\nvc3-application\n\n\n\n\nOne of the supported middleware applications to be deployed as an overlay defining a virtual cluster.\n\n\n\n\nvc3-builder\n\n\n\n\nPilot-like executable that prepares an environment for middleware and user applications. The vc3-builder is a pilot-like utility, submitted to resource targets, which programmatically satisfies all requested dependencies before handing off control to the middleware layer. Its special feature is the ability to satisfy dependencies in different ways on different targets, depending on what it finds, e.g. it can tell if a dep is already satisfied, can download a pre-built library, or dynamically compile a dep if needed. Several builders can simultaneously satisfy dependencies in parallel on a resource (provided a shared filesystem).\n\n\n\n\nvc3-client\n\n\n\n\nPackage containing the VC3-aware library for creating, listing, updating, and deleting entities within the infoservice. It also contains a command line interface to the library.\n\n\n\n\nvc3-core\n\n\n\n\nThe VC3 component that coordinates activity within the dynamic infrastructure. One vc3-core exists per virtual cluster Request during its lifecycle. A vc3-core will typically start a vc3-factory, along with any central components the cluster will need (e.g. an HTCondor collector/negotiator/schedd, a WorkQueue catalog, or a Squid server).\n\n\n\n\nvc3-master\n\n\n\n\nPackage containing the long-running daemon, running on the static infrastructure, that manages the lifecycle of all virtual cluster Requests. The vc3-master is a long-running daemon, running on the static infrastructure, that manages the lifecycle of all virtual cluster Requests. It polls the infoservice for new Requests, and spawns vc3-core instances on the dynamic infrastructure to service them. It also handles the generation and processing of all derived entities within the infoservice tree.\n\n\n\n\nvc3-release\n\n\n\n\nThis is a developer package that contains various setup and test utilities, and artifacts needed to create and use a YUM RPM repository.\n\n\n\n\nvc3-resource-tool\n\n\n\n\nThe vc3-resource-tool is a utility to be run by end users on resource targets in order to pair and enable them for usage by the VC3 system.", 
            "title": "Glossary"
        }, 
        {
            "location": "/glossary/#glossary-and-concepts-used-in-vc3-documentation", 
            "text": "", 
            "title": "Glossary and Concepts used in VC3 Documentation"
        }, 
        {
            "location": "/glossary/#portal-users", 
            "text": "", 
            "title": "Portal Users"
        }, 
        {
            "location": "/glossary/#allocation", 
            "text": "An Allocation refers to both User and Resource concepts. Each Allocation must be owned by an User. Allocations are divisible/fractionable, and can be given to Projects. Allocations may not be oversubscribed. However, unbounded Allocations may be parents of multiple unbounded SubAllocations. Bounded Allocations cannot spawn unbounded SubAllocations. If a Resource grants hard allocation and allows backfill mode, those are two distinct Allocations (one hard and one unbounded)", 
            "title": "Allocation"
        }, 
        {
            "location": "/glossary/#authentication", 
            "text": "The current mechanism for users to sign-up and create accounts on the VC3 platform is by authenticating themselves with their  GlobusID .", 
            "title": "Authentication"
        }, 
        {
            "location": "/glossary/#mfa", 
            "text": "Multi-factor authentication, c.f.  Wikipedia .", 
            "title": "MFA"
        }, 
        {
            "location": "/glossary/#project", 
            "text": "A collection of \u201cAllocations\u201d from individual users. It has at least 1 \u201cuser owner\u201d, and 0 or more non-owner members. The owner is also a member.", 
            "title": "Project"
        }, 
        {
            "location": "/glossary/#request", 
            "text": "Entity that encapsulates all information that defines a particular virtual cluster. Creating a new Request triggers creation of the cluster.", 
            "title": "Request"
        }, 
        {
            "location": "/glossary/#request-templates", 
            "text": "a list of pre-existing forms to be used as base for new cluster requests creation.", 
            "title": "Request templates"
        }, 
        {
            "location": "/glossary/#resource", 
            "text": "Any target on which a vc3-builder will run to provide computing power to a virtual cluster.", 
            "title": "Resource"
        }, 
        {
            "location": "/glossary/#resource-profiles", 
            "text": "a list of pre-existing forms to be used as base for new resource definition.", 
            "title": "Resource profiles"
        }, 
        {
            "location": "/glossary/#service-unit", 
            "text": "Service units are essentially just walltime hours, with minimum charges based on minimum cores or minimum nodes per job. Much like HEPSPEC, the SUs can be normalized/converted based on LINPACK benchmarks. Doc from XSEDE: https://portal.xsede.org/knowledge-base/-/kb/document/bazo    For storage, possibly with multiple allocations per user, examples are scratch disk vs long term storage.    Exotic devices like GPUs may or may not be accounted for, depending on the resource.", 
            "title": "Service unit"
        }, 
        {
            "location": "/glossary/#sub-allocation", 
            "text": "A SubAllocation can be defined in terms of fraction or units (cpuhours?, $dollars, HEPSPEC) or be unbounded. SubAllocations are children of an Allocation.", 
            "title": "Sub-Allocation"
        }, 
        {
            "location": "/glossary/#user", 
            "text": "Every User has 0 or more Allocations. Users are owners or members of one or more projects. A User in a project can make Request(s) utilizing project member\u2019s Allocations", 
            "title": "User"
        }, 
        {
            "location": "/glossary/#project-developers", 
            "text": "", 
            "title": "Project Developers"
        }, 
        {
            "location": "/glossary/#cluster-states", 
            "text": "List of each possible state of a cluster throughout its lifecycle:   NEW - Request was just created.  VALIDATED - Request has been validated for basic correctness.  PENDING - Request is valid and is waiting to be instantiated.  GROWING - Cluster is in the process of being instantiated but is not yet usable.  RUNNING - Cluster is ready to use.  SHRINKING - Cluster resources are being removed.  TERMINATING - Cluster is about to be destroyed.  TERMINATED - Cluster no longer exists.", 
            "title": "cluster states"
        }, 
        {
            "location": "/glossary/#credible", 
            "text": "Credible is a 3rd-party utility for programmatically generating, storing, and retrieving security tokens.", 
            "title": "credible"
        }, 
        {
            "location": "/glossary/#dynamic-infrastructure", 
            "text": "Services that are instantiated upon a virtual cluster request, such as the factory.", 
            "title": "dynamic infrastructure"
        }, 
        {
            "location": "/glossary/#factory", 
            "text": "The scheduler and resource manager for middleware.", 
            "title": "factory"
        }, 
        {
            "location": "/glossary/#formatter", 
            "text": "A plugin that augments the output of |Flake8| when passed to flake8 --format.", 
            "title": "formatter"
        }, 
        {
            "location": "/glossary/#info-service", 
            "text": "Long-running daemon that interacts with the information database on behalf of other services.    The VC3 info service serves as both a persistence mechanism for the overall service, and a message bus between components. Information is stored and retrieved in the form of JSON-formatted documents, which thus form a single tree of information entities/nodes. The service optionally allows access security by enforcing ACLs at each node level.", 
            "title": "info service"
        }, 
        {
            "location": "/glossary/#pin", 
            "text": "Personal Identification Number. One-time password for configuring a VC3 resource via vc3-resource-tool", 
            "title": "PIN"
        }, 
        {
            "location": "/glossary/#plugin-manager", 
            "text": "The plugin manager is a 3rd-party small utility for quickly constructing plugin objects from configuration input.", 
            "title": "plugin-manager"
        }, 
        {
            "location": "/glossary/#request-id", 
            "text": "Unique identifier for a virtual cluster request.", 
            "title": "request ID"
        }, 
        {
            "location": "/glossary/#static-infrastructure", 
            "text": "A set of long-running services, such as the Info Service, Master, etc.", 
            "title": "static infrastructure"
        }, 
        {
            "location": "/glossary/#vc3-application", 
            "text": "One of the supported middleware applications to be deployed as an overlay defining a virtual cluster.", 
            "title": "vc3-application"
        }, 
        {
            "location": "/glossary/#vc3-builder", 
            "text": "Pilot-like executable that prepares an environment for middleware and user applications. The vc3-builder is a pilot-like utility, submitted to resource targets, which programmatically satisfies all requested dependencies before handing off control to the middleware layer. Its special feature is the ability to satisfy dependencies in different ways on different targets, depending on what it finds, e.g. it can tell if a dep is already satisfied, can download a pre-built library, or dynamically compile a dep if needed. Several builders can simultaneously satisfy dependencies in parallel on a resource (provided a shared filesystem).", 
            "title": "vc3-builder"
        }, 
        {
            "location": "/glossary/#vc3-client", 
            "text": "Package containing the VC3-aware library for creating, listing, updating, and deleting entities within the infoservice. It also contains a command line interface to the library.", 
            "title": "vc3-client"
        }, 
        {
            "location": "/glossary/#vc3-core", 
            "text": "The VC3 component that coordinates activity within the dynamic infrastructure. One vc3-core exists per virtual cluster Request during its lifecycle. A vc3-core will typically start a vc3-factory, along with any central components the cluster will need (e.g. an HTCondor collector/negotiator/schedd, a WorkQueue catalog, or a Squid server).", 
            "title": "vc3-core"
        }, 
        {
            "location": "/glossary/#vc3-master", 
            "text": "Package containing the long-running daemon, running on the static infrastructure, that manages the lifecycle of all virtual cluster Requests. The vc3-master is a long-running daemon, running on the static infrastructure, that manages the lifecycle of all virtual cluster Requests. It polls the infoservice for new Requests, and spawns vc3-core instances on the dynamic infrastructure to service them. It also handles the generation and processing of all derived entities within the infoservice tree.", 
            "title": "vc3-master"
        }, 
        {
            "location": "/glossary/#vc3-release", 
            "text": "This is a developer package that contains various setup and test utilities, and artifacts needed to create and use a YUM RPM repository.", 
            "title": "vc3-release"
        }, 
        {
            "location": "/glossary/#vc3-resource-tool", 
            "text": "The vc3-resource-tool is a utility to be run by end users on resource targets in order to pair and enable them for usage by the VC3 system.", 
            "title": "vc3-resource-tool"
        }
    ]
}